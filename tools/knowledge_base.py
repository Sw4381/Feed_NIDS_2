# -*- coding: utf-8 -*-
"""
Knowledge Base Manager with FAISS Vector Store Integration
Train ì‹œì ì˜ labeled ì‚¬ë¡€ë“¤ì„ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ
"""
from __future__ import annotations
import os
import glob
import pickle
import hashlib
from typing import Optional, List, Dict, Tuple
import numpy as np
import pandas as pd
from tools.base import get_logger

log = get_logger("KnowledgeBase")

# ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ Feature 22ê°œ (ê³ ì •)
NETWORK_SECURITY_FEATURES = [
    'alpahbet_cnt_payload_sum',      # Payload ì•ŒíŒŒë²³ ë¬¸ìž ìˆ˜
    'alpha_cnt_dns_query_sum',       # DNS ì¿¼ë¦¬ ì•ŒíŒŒë²³ ë¬¸ìž ìˆ˜
    'client_extensions_cnt',          # í´ë¼ì´ì–¸íŠ¸ í™•ìž¥ ìˆ˜
    'entropys_avg',                   # ì—”íŠ¸ë¡œí”¼ í‰ê· 
    'flow_delta_times_sum',          # Flow ì‹œê°„ ë¸íƒ€ í•©
    'flow_duration_seconds',         # Flow ì§€ì† ì‹œê°„ (ì´ˆ)
    'flow_stdev_time',               # Flow ì‹œê°„ í‘œì¤€íŽ¸ì°¨
    'nonascii_cnt_dns_query_sum',    # DNS ì¿¼ë¦¬ ë¹„ASCII ë¬¸ìž ìˆ˜
    'nonascii_cnt_payload_sum',      # Payload ë¹„ASCII ë¬¸ìž ìˆ˜
    'number_cnt_dns_query_sum',      # DNS ì¿¼ë¦¬ ìˆ«ìž ë¬¸ìž ìˆ˜
    'number_cnt_payload_sum',        # Payload ìˆ«ìž ë¬¸ìž ìˆ˜
    'payload_len_max',               # Payload ìµœëŒ€ ê¸¸ì´
    'payload_len_min',               # Payload ìµœì†Œ ê¸¸ì´
    'payload_lens_sum',              # Payload ê¸¸ì´ í•©
    'payload_packets_cnt',           # Payload íŒ¨í‚· ìˆ˜
    'query_response_ttls_sum',       # DNS TTL í•©
    'server_certificates_cnt',        # ì„œë²„ ì¸ì¦ì„œ ìˆ˜
    'server_extensions_cnt',          # ì„œë²„ í™•ìž¥ ìˆ˜
    'special_cnt_dns_query_sum',     # DNS ì¿¼ë¦¬ íŠ¹ìˆ˜ë¬¸ìž ìˆ˜
    'special_cnt_payload_sum',       # Payload íŠ¹ìˆ˜ë¬¸ìž ìˆ˜
    'tls_SAN_cnt',                   # TLS SAN ìˆ˜
    'total_packets_cnt'              # ì´ íŒ¨í‚· ìˆ˜
]

# FAISS ê°€ìš©ì„± ì²´í¬
try:
    import faiss
    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False
    log.warning("âš ï¸ FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ê¸°ëŠ¥ ë¹„í™œì„±í™” (pip install faiss-cpu)")

# StandardScaler
try:
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    log.warning("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜ - ì •ê·œí™” ì—†ì´ ì§„í–‰ (pip install scikit-learn)")


class KnowledgeBase:
    """
    Train_Cases ë””ë ‰í† ë¦¬ì—ì„œ labeled ì‚¬ë¡€ë¥¼ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬
    
    Features:
    - ê¸°ì¡´ KB ë¡œë“œ ë° ê´€ë¦¬ (backward compatible)
    - FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
    - ìºì‹œ ì‹œìŠ¤í…œ (hash ê¸°ë°˜)
    - GPU ì§€ì› (ì„ íƒ ì‚¬í•­)
    """
    
    def __init__(self, 
                 train_cases_dir: str = "/Train_Cases",
                 use_faiss: bool = True,
                 cache_dir: str = "./cache",
                 index_type: str = "IVF",
                 n_clusters: int = 100,
                 use_gpu: bool = False):
        """
        Args:
            train_cases_dir: Train Cases ë””ë ‰í† ë¦¬ ê²½ë¡œ
            use_faiss: FAISS ë²¡í„° ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            index_type: FAISS ì¸ë±ìŠ¤ íƒ€ìž… ("Flat", "IVF", "HNSW")
            n_clusters: IVF í´ëŸ¬ìŠ¤í„° ìˆ˜
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.train_cases_dir = train_cases_dir
        self.kb_df = None
        self.is_loaded = False
        
        # FAISS ì„¤ì •
        self.use_faiss = use_faiss and HAS_FAISS
        self.cache_dir = cache_dir
        self.index_type = index_type
        self.n_clusters = n_clusters
        self.use_gpu = use_gpu and HAS_FAISS
        
        # FAISS ë²¡í„° ì €ìž¥ì†Œ
        self.faiss_index = None
        self.scaler = None
        self.feature_columns = []
        self.shap_feature_vocab = []
        self.metadata = []
        self.faiss_built = False
        
        if self.use_faiss:
            os.makedirs(cache_dir, exist_ok=True)

    def load(self) -> bool:
        """
        Train_Cases ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ
        FAISSê°€ í™œì„±í™”ë˜ì–´ ìžˆìœ¼ë©´ ìžë™ìœ¼ë¡œ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Returns: ì„±ê³µ ì—¬ë¶€
        """
        if not os.path.exists(self.train_cases_dir):
            log.warning(f"Train_Cases ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.train_cases_dir}")
            return False

        files = sorted(glob.glob(os.path.join(self.train_cases_dir, "*.csv")))
        if not files:
            log.warning(f"Train_Casesì— CSV íŒŒì¼ ì—†ìŒ: {self.train_cases_dir}")
            return False

        dfs = []
        for f in files:
            try:
                df = pd.read_csv(f, low_memory=False)
                if "label" in df.columns and len(df) > 0:
                    dfs.append(df)
                    log.info(f"ë¡œë“œ: {os.path.basename(f)} ({len(df)} rows)")
            except Exception as e:
                log.warning(f"ë¡œë“œ ì‹¤íŒ¨: {f} â†’ {e}")
                continue

        if not dfs:
            log.warning("ë¡œë“œëœ labeled ì‚¬ë¡€ ì—†ìŒ")
            return False

        self.kb_df = pd.concat(dfs, ignore_index=True)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required = ["label"]
        missing = [c for c in required if c not in self.kb_df.columns]
        if missing:
            log.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return False

        self.is_loaded = True
        log.info(f"âœ… Knowledge Base ë¡œë“œ ì™„ë£Œ: {len(self.kb_df)} rows")
        
        # FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶•
        if self.use_faiss:
            log.info("\n" + "=" * 60)
            log.info("FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• ì‹œìž‘")
            log.info("=" * 60)
            success = self.build_faiss_index()
            if success:
                log.info("âœ… FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            else:
                log.warning("âš ï¸ FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨ - ê¸°ë³¸ KBë§Œ ì‚¬ìš©")
        
        return True

    def build_faiss_index(self, feature_cols: List[str] = None, 
                          top_k: int = 5, force_rebuild: bool = False) -> bool:
        """
        FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
        
        Args:
            feature_cols: Feature ì»¬ëŸ¼ (Noneì´ë©´ ê¸°ë³¸ 22ê°œ ì‚¬ìš©)
            top_k: SHAP Top-K
            force_rebuild: ê°•ì œ ìž¬êµ¬ì¶•
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.is_loaded or self.kb_df is None:
            log.error("KBê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return False
        
        if not HAS_FAISS:
            log.warning("FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ìŠ¤í‚µ")
            return False
        
        # Feature ì»¬ëŸ¼ ê²°ì •
        if feature_cols is None:
            feature_cols = NETWORK_SECURITY_FEATURES
        
        # KBì— ì‹¤ì œ ì¡´ìž¬í•˜ëŠ” Featureë§Œ í•„í„°ë§
        available_features = [f for f in feature_cols if f in self.kb_df.columns]
        missing_features = [f for f in feature_cols if f not in self.kb_df.columns]
        
        if missing_features:
            log.warning(f"ëˆ„ë½ëœ Feature ({len(missing_features)}ê°œ)")
        
        if not available_features:
            log.error("ì‚¬ìš© ê°€ëŠ¥í•œ Feature ì—†ìŒ")
            return False
        
        log.info(f"ì‚¬ìš© Feature: {len(available_features)}/{len(feature_cols)}")
        
        # ìºì‹œ í™•ì¸
        data_hash = self._compute_data_hash(self.kb_df, available_features)
        cache_paths = self._get_cache_paths(data_hash)
        
        if not force_rebuild and all(os.path.exists(p) for p in cache_paths.values()):
            log.info(f"ðŸ“¦ ìºì‹œì—ì„œ ë¡œë“œ (hash={data_hash[:8]})")
            try:
                return self._load_from_cache(cache_paths)
            except Exception as e:
                log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìž¬êµ¬ì¶•: {e}")
        
        log.info(f"ðŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # Step 1: Feature ë²¡í„° ì¶”ì¶œ ë° ì •ê·œí™”
        feature_matrix = self.kb_df[available_features].values.astype(np.float32)
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)
        
        if HAS_SKLEARN:
            self.scaler = StandardScaler()
            normalized_features = self.scaler.fit_transform(feature_matrix).astype(np.float32)
        else:
            self.scaler = None
            normalized_features = feature_matrix
        
        # Step 2: SHAP Top-K â†’ One-Hot ì¸ì½”ë”©
        all_shap_features = set()
        shap_feature_lists = []
        
        for idx, row in self.kb_df.iterrows():
            row_shap_features = []
            for i in range(1, top_k + 1):
                feat_col = f"shap_top{i}_feature"
                if feat_col in row:
                    feat_name = str(row[feat_col])
                    if feat_name and feat_name not in ("", "nan", "None"):
                        row_shap_features.append(feat_name)
                        all_shap_features.add(feat_name)
            shap_feature_lists.append(row_shap_features)
        
        self.shap_feature_vocab = sorted(all_shap_features)
        shap_to_idx = {feat: i for i, feat in enumerate(self.shap_feature_vocab)}
        
        shap_onehot_matrix = np.zeros((len(self.kb_df), len(self.shap_feature_vocab)), dtype=np.float32)
        for row_idx, shap_feats in enumerate(shap_feature_lists):
            for feat_name in shap_feats:
                feat_idx = shap_to_idx[feat_name]
                shap_onehot_matrix[row_idx, feat_idx] = 1.0
        
        # Step 3: ë³µí•© ë²¡í„° ìƒì„±
        composite_vectors = np.hstack([normalized_features, shap_onehot_matrix]).astype(np.float32)
        
        log.info(f"  â”œâ”€ Feature ì°¨ì›: {normalized_features.shape[1]}")
        log.info(f"  â”œâ”€ SHAP Vocab ì°¨ì›: {shap_onehot_matrix.shape[1]}")
        log.info(f"  â””â”€ ì´ ì°¨ì›: {composite_vectors.shape[1]}")
        
        # Step 4: ë©”íƒ€ë°ì´í„° ì €ìž¥
        self.metadata = []
        for idx, row in self.kb_df.iterrows():
            meta = {
                "idx": idx,
                "A_ip": str(row.get("A_ip", "")),
                "B_ip": str(row.get("B_ip", "")),
                "label": str(row.get("label", "")),
                "case_id": str(row.get("case_id", "")),
                "shap_features": {}
            }
            for i in range(1, top_k + 1):
                feat_col = f"shap_top{i}_feature"
                if feat_col in row:
                    feat_name = str(row[feat_col])
                    if feat_name and feat_name not in ("", "nan", "None"):
                        meta["shap_features"][feat_name] = i
            self.metadata.append(meta)
        
        # Step 5: FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = composite_vectors.shape[1]
        self.feature_columns = available_features
        
        self.faiss_index = self._create_faiss_index(dimension)
        
        if self.index_type == "IVF":
            self.faiss_index.train(composite_vectors)
        
        self.faiss_index.add(composite_vectors)
        self.faiss_built = True
        
        # ìºì‹œ ì €ìž¥
        try:
            self._save_to_cache(cache_paths, composite_vectors)
        except Exception as e:
            log.warning(f"ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")
        
        return True

    def _create_faiss_index(self, dimension: int):
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        if self.index_type == "Flat":
            index = faiss.IndexFlatL2(dimension)
        elif self.index_type == "IVF":
            quantizer = faiss.IndexFlatL2(dimension)
            n_clusters = min(self.n_clusters, max(1, len(self.metadata) // 10))
            index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)
        elif self.index_type == "HNSW":
            index = faiss.IndexHNSWFlat(dimension, 32)
            index.hnsw.efConstruction = 40
        else:
            index = faiss.IndexFlatL2(dimension)
        
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(res, 0, index)
            except Exception as e:
                log.warning(f"GPU ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        return index

    def search_similar_cases(self, query_features: np.ndarray, 
                            query_shap_features: Dict[str, int],
                            k: int = 100) -> List[Tuple[int, float]]:
        """
        FAISSë¡œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query_features: ì¿¼ë¦¬ feature ë²¡í„° (ì •ê·œí™” ì „)
            query_shap_features: ì¿¼ë¦¬ SHAP features {name: rank}
            k: ë°˜í™˜í•  í›„ë³´ ìˆ˜
            
        Returns:
            [(kb_index, distance), ...]
        """
        if not self.faiss_built:
            log.warning("FAISS ì¸ë±ìŠ¤ ë¯¸êµ¬ì¶•")
            return []
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        query_features = np.nan_to_num(query_features, nan=0.0).astype(np.float32)
        if self.scaler is not None:
            normalized_query = self.scaler.transform(query_features.reshape(1, -1)).astype(np.float32)
        else:
            normalized_query = query_features.reshape(1, -1).astype(np.float32)
        
        # SHAP One-Hot
        query_shap_onehot = np.zeros((1, len(self.shap_feature_vocab)), dtype=np.float32)
        for feat_name in query_shap_features.keys():
            if feat_name in self.shap_feature_vocab:
                feat_idx = self.shap_feature_vocab.index(feat_name)
                query_shap_onehot[0, feat_idx] = 1.0
        
        # ë³µí•© ë²¡í„°
        query_composite = np.hstack([normalized_query, query_shap_onehot]).astype(np.float32)
        
        # ê²€ìƒ‰
        if self.index_type == "IVF":
            self.faiss_index.nprobe = min(10, self.n_clusters)
        
        distances, indices = self.faiss_index.search(query_composite, k)
        
        results = []
        for i in range(len(indices[0])):
            idx = int(indices[0][i])
            dist = float(distances[0][i])
            if idx >= 0:
                results.append((idx, dist))
        
        return results

    def get_labeled_cases(self, labels: List[str] = None) -> pd.DataFrame:
        """
        íŠ¹ì • ë¼ë²¨ì˜ ì‚¬ë¡€ë§Œ ë°˜í™˜ (backward compatible)
        """
        if not self.is_loaded or self.kb_df is None:
            return pd.DataFrame()
        if labels is None:
            return self.kb_df.copy()
        return self.kb_df[self.kb_df["label"].isin(labels)].copy()

    def get_stats(self) -> Dict[str, any]:
        """Knowledge Base í†µê³„ ë°˜í™˜"""
        if not self.is_loaded or self.kb_df is None:
            return {}
        
        stats = {
            "total": len(self.kb_df),
            "labels": self.kb_df["label"].value_counts().to_dict() if "label" in self.kb_df.columns else {},
            "files_loaded": len(glob.glob(os.path.join(self.train_cases_dir, "*.csv"))),
            "faiss_enabled": self.faiss_built,
        }
        
        if self.faiss_built:
            stats["faiss_index_type"] = self.index_type
            stats["faiss_dimension"] = len(self.feature_columns) + len(self.shap_feature_vocab)
            stats["feature_dim"] = len(self.feature_columns)
            stats["shap_vocab_dim"] = len(self.shap_feature_vocab)
        
        return stats

    def export_as_feedback_corpus(self, out_dir: str = "./feedback_cases") -> str:
        """
        Knowledge Baseë¥¼ í”¼ë“œë°± ì½”í¼ìŠ¤ í˜•ì‹ìœ¼ë¡œ Export (backward compatible)
        """
        if not self.is_loaded or self.kb_df is None:
            log.error("Knowledge Base ë¯¸ë¡œë“œ")
            return ""

        os.makedirs(out_dir, exist_ok=True)
        export_df = self.kb_df.copy()
        
        for col, default in [
            ("case_id", ""),
            ("feedback_label", ""),
            ("feedback_confidence", ""),
            ("feedback_reason", ""),
            ("reviewed", True),
            ("needs_review", False),
            ("review_date", ""),
        ]:
            if col not in export_df.columns:
                if col == "reviewed":
                    export_df[col] = True
                elif col == "case_id" and "case_id" not in export_df.columns:
                    export_df[col] = [f"KB_{i:06d}" for i in range(len(export_df))]
                elif col == "feedback_label":
                    export_df[col] = export_df.get("label", "")
                elif col == "feedback_confidence":
                    export_df[col] = 5
                elif col == "feedback_reason":
                    export_df[col] = "(Train Knowledge Base)"
                else:
                    export_df[col] = default

        out_path = os.path.join(out_dir, "Knowledge_Base_Corpus.csv")
        export_df.to_csv(out_path, index=False, encoding="utf-8")
        log.info(f"Knowledge Base Export: {out_path} ({len(export_df)} rows)")
        return out_path

    # ========================================
    # ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    # ========================================
    
    def _compute_data_hash(self, kb_df: pd.DataFrame, feature_cols: List[str]) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        key_parts = [
            f"rows:{len(kb_df)}",
            f"cols:{len(feature_cols)}",
            f"features:{','.join(sorted(feature_cols))}",
        ]
        if len(kb_df) > 0:
            sample_size = min(100, len(kb_df))
            sample_idx = np.linspace(0, len(kb_df)-1, sample_size, dtype=int)
            sample_data = kb_df.iloc[sample_idx][feature_cols].values
            data_hash = hashlib.md5(sample_data.tobytes()).hexdigest()[:16]
            key_parts.append(f"data:{data_hash}")
        return hashlib.md5("_".join(key_parts).encode()).hexdigest()
    
    def _get_cache_paths(self, data_hash: str) -> Dict[str, str]:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
        prefix = os.path.join(self.cache_dir, f"kb_{data_hash}")
        return {
            "index": f"{prefix}_index.faiss",
            "scaler": f"{prefix}_scaler.pkl",
            "metadata": f"{prefix}_metadata.pkl",
            "config": f"{prefix}_config.pkl",
        }
    
    def _save_to_cache(self, cache_paths: Dict[str, str], vectors: np.ndarray):
        """ìºì‹œ ì €ìž¥"""
        if self.use_gpu:
            cpu_index = faiss.index_gpu_to_cpu(self.faiss_index)
            faiss.write_index(cpu_index, cache_paths["index"])
        else:
            faiss.write_index(self.faiss_index, cache_paths["index"])
        
        if self.scaler is not None:
            with open(cache_paths["scaler"], "wb") as f:
                pickle.dump(self.scaler, f)
        
        with open(cache_paths["metadata"], "wb") as f:
            pickle.dump(self.metadata, f)
        
        config = {
            "feature_columns": self.feature_columns,
            "shap_feature_vocab": self.shap_feature_vocab,
            "index_type": self.index_type,
            "n_clusters": self.n_clusters,
        }
        with open(cache_paths["config"], "wb") as f:
            pickle.dump(config, f)
    
    def _load_from_cache(self, cache_paths: Dict[str, str]) -> bool:
        """ìºì‹œ ë¡œë“œ"""
        with open(cache_paths["config"], "rb") as f:
            config = pickle.load(f)
        
        self.feature_columns = config["feature_columns"]
        self.shap_feature_vocab = config.get("shap_feature_vocab", [])
        
        self.faiss_index = faiss.read_index(cache_paths["index"])
        
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.faiss_index = faiss.index_cpu_to_gpu(res, 0, self.faiss_index)
            except:
                pass
        
        if os.path.exists(cache_paths["scaler"]):
            with open(cache_paths["scaler"], "rb") as f:
                self.scaler = pickle.load(f)
        
        with open(cache_paths["metadata"], "rb") as f:
            self.metadata = pickle.load(f)
        
        self.faiss_built = True
        return True

    def __len__(self) -> int:
        return len(self.kb_df) if self.is_loaded and self.kb_df is not None else 0

    def __repr__(self) -> str:
        if not self.is_loaded:
            return "KnowledgeBase(not loaded)"
        stats = self.get_stats()
        faiss_info = f", FAISS({self.index_type})" if self.faiss_built else ""
        return f"KnowledgeBase(total={stats.get('total', 0)}, labels={stats.get('labels', {})}{faiss_info})"