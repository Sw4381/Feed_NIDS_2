# -*- coding: utf-8 -*-
"""
Knowledge Base Manager with FAISS Vector Store Integration (GPU ìµœì í™”)
Train ì‹œì ì˜ labeled ì‚¬ë¡€ë“¤ì„ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ
"""
from __future__ import annotations
import os
import glob
import pickle
import hashlib
import time
from typing import Optional, List, Dict, Tuple
import numpy as np
import pandas as pd
from tools.base import get_logger

log = get_logger("KnowledgeBase")

# ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ Feature 22ê°œ (ê³ ì •)
NETWORK_SECURITY_FEATURES = [
    'alpahbet_cnt_payload_sum',      # Payload ì•ŒíŒŒë²³ ë¬¸ìž ìˆ˜
    'alpha_cnt_dns_query_sum',       # DNS ì¿¼ë¦¬ ì•ŒíŒŒë²³ ë¬¸ìž ìˆ˜
    'client_extensions_cnt',          # í´ë¼ì´ì–¸íŠ¸ í™•ìž¥ ìˆ˜
    'entropys_avg',                   # ì—”íŠ¸ë¡œí”¼ í‰ê· 
    'flow_delta_times_sum',          # Flow ì‹œê°„ ë¸íƒ€ í•©
    'flow_duration_seconds',         # Flow ì§€ì† ì‹œê°„ (ì´ˆ)
    'flow_stdev_time',               # Flow ì‹œê°„ í‘œì¤€íŽ¸ì°¨
    'nonascii_cnt_dns_query_sum',    # DNS ì¿¼ë¦¬ ë¹„ASCII ë¬¸ìž ìˆ˜
    'nonascii_cnt_payload_sum',      # Payload ë¹„ASCII ë¬¸ìž ìˆ˜
    'number_cnt_dns_query_sum',      # DNS ì¿¼ë¦¬ ìˆ«ìž ë¬¸ìž ìˆ˜
    'number_cnt_payload_sum',        # Payload ìˆ«ìž ë¬¸ìž ìˆ˜
    'payload_len_max',               # Payload ìµœëŒ€ ê¸¸ì´
    'payload_len_min',               # Payload ìµœì†Œ ê¸¸ì´
    'payload_lens_sum',              # Payload ê¸¸ì´ í•©
    'payload_packets_cnt',           # Payload íŒ¨í‚· ìˆ˜
    'query_response_ttls_sum',       # DNS TTL í•©
    'server_certificates_cnt',        # ì„œë²„ ì¸ì¦ì„œ ìˆ˜
    'server_extensions_cnt',          # ì„œë²„ í™•ìž¥ ìˆ˜
    'special_cnt_dns_query_sum',     # DNS ì¿¼ë¦¬ íŠ¹ìˆ˜ë¬¸ìž ìˆ˜
    'special_cnt_payload_sum',       # Payload íŠ¹ìˆ˜ë¬¸ìž ìˆ˜
    'tls_SAN_cnt',                   # TLS SAN ìˆ˜
    'total_packets_cnt'              # ì´ íŒ¨í‚· ìˆ˜
]

# FAISS ê°€ìš©ì„± ì²´í¬
try:
    import faiss
    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False
    log.warning("âš ï¸ FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ê¸°ëŠ¥ ë¹„í™œì„±í™” (pip install faiss-cpu)")

# StandardScaler
try:
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    log.warning("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜ - ì •ê·œí™” ì—†ì´ ì§„í–‰ (pip install scikit-learn)")


class KnowledgeBase:
    """
    Train_Cases ë””ë ‰í† ë¦¬ì—ì„œ labeled ì‚¬ë¡€ë¥¼ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬
    
    Features:
    - ê¸°ì¡´ KB ë¡œë“œ ë° ê´€ë¦¬ (backward compatible)
    - FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
    - ìºì‹œ ì‹œìŠ¤í…œ (hash ê¸°ë°˜)
    - GPU ì§€ì› (ì„ íƒ ì‚¬í•­)
    """
    
    def __init__(self, 
                 train_cases_dir: str = "/Train_Cases",
                 use_faiss: bool = True,
                 cache_dir: str = "./cache",
                 index_type: str = "IVF",
                 n_clusters: int = 100,
                 use_gpu: bool = False):
        """
        Args:
            train_cases_dir: Train Cases ë””ë ‰í† ë¦¬ ê²½ë¡œ
            use_faiss: FAISS ë²¡í„° ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            index_type: FAISS ì¸ë±ìŠ¤ íƒ€ìž… ("Flat", "IVF", "HNSW")
            n_clusters: IVF í´ëŸ¬ìŠ¤í„° ìˆ˜
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.train_cases_dir = train_cases_dir
        self.kb_df = None
        self.is_loaded = False
        
        # FAISS ì„¤ì •
        self.use_faiss = use_faiss and HAS_FAISS
        self.cache_dir = cache_dir
        self.index_type = index_type
        self.n_clusters = n_clusters
        self.use_gpu = use_gpu and HAS_FAISS
        
        # FAISS ë²¡í„° ì €ìž¥ì†Œ
        self.faiss_index = None
        self.scaler = None
        self.feature_columns = []
        self.shap_feature_vocab = []
        self.metadata = []
        self.faiss_built = False
        
        # GPU ë¦¬ì†ŒìŠ¤ (ìž¬ì‚¬ìš©)
        self.gpu_resources = None
        
        if self.use_faiss:
            os.makedirs(cache_dir, exist_ok=True)

    def load(self) -> bool:
        """
        Train_Cases ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ
        FAISSê°€ í™œì„±í™”ë˜ì–´ ìžˆìœ¼ë©´ ìžë™ìœ¼ë¡œ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Returns: ì„±ê³µ ì—¬ë¶€
        """
        if not os.path.exists(self.train_cases_dir):
            log.warning(f"Train_Cases ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.train_cases_dir}")
            return False

        files = sorted(glob.glob(os.path.join(self.train_cases_dir, "*.csv")))
        if not files:
            log.warning(f"Train_Casesì— CSV íŒŒì¼ ì—†ìŒ: {self.train_cases_dir}")
            return False

        dfs = []
        for f in files:
            try:
                df = pd.read_csv(f, low_memory=False)
                if "label" in df.columns and len(df) > 0:
                    dfs.append(df)
                    log.info(f"ë¡œë“œ: {os.path.basename(f)} ({len(df)} rows)")
            except Exception as e:
                log.warning(f"ë¡œë“œ ì‹¤íŒ¨: {f} â†’ {e}")
                continue

        if not dfs:
            log.warning("ë¡œë“œëœ labeled ì‚¬ë¡€ ì—†ìŒ")
            return False

        self.kb_df = pd.concat(dfs, ignore_index=True)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required = ["label"]
        missing = [c for c in required if c not in self.kb_df.columns]
        if missing:
            log.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return False

        self.is_loaded = True
        log.info(f"âœ… Knowledge Base ë¡œë“œ ì™„ë£Œ: {len(self.kb_df)} rows")
        
        # FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶•
        if self.use_faiss:
            log.info("\n" + "=" * 70)
            log.info("FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• ì‹œìž‘")
            log.info("=" * 70)
            success = self.build_faiss_index()
            if success:
                log.info("âœ… FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            else:
                log.warning("âš ï¸ FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨ - ê¸°ë³¸ KBë§Œ ì‚¬ìš©")
        
        return True

    def build_faiss_index(self, feature_cols: List[str] = None, 
                        top_k: int = 5, force_rebuild: bool = False) -> bool:
        """
        FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
        """
        if not self.is_loaded or self.kb_df is None:
            log.error("KBê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return False
        
        if not HAS_FAISS:
            log.warning("FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ìŠ¤í‚µ")
            return False
        
        # Feature ì»¬ëŸ¼ ê²°ì •
        if feature_cols is None:
            feature_cols = NETWORK_SECURITY_FEATURES
        
        available_features = [f for f in feature_cols if f in self.kb_df.columns]
        missing_features = [f for f in feature_cols if f not in self.kb_df.columns]
        
        if missing_features:
            log.warning(f"ëˆ„ë½ëœ Feature ({len(missing_features)}ê°œ)")
        
        if not available_features:
            log.error("ì‚¬ìš© ê°€ëŠ¥í•œ Feature ì—†ìŒ")
            return False
        
        log.info(f"ðŸ“Š ì‚¬ìš© Feature: {len(available_features)}/{len(feature_cols)}")
        log.info(f"ðŸŽ® GPU ì‚¬ìš©: {'ON' if self.use_gpu else 'OFF'}")
        log.info(f"ðŸ”§ ì¸ë±ìŠ¤ íƒ€ìž…: {self.index_type}")
        log.info(f"ðŸ“¦ KB í¬ê¸°: {len(self.kb_df)}ê°œ")
        
        # ìºì‹œ í™•ì¸
        data_hash = self._compute_data_hash(self.kb_df, available_features)
        cache_paths = self._get_cache_paths(data_hash)
        
        if not force_rebuild and all(os.path.exists(p) for p in cache_paths.values()):
            log.info(f"ðŸ“¦ ìºì‹œì—ì„œ ë¡œë“œ (hash={data_hash[:8]})")
            try:
                return self._load_from_cache(cache_paths)
            except Exception as e:
                log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìž¬êµ¬ì¶•: {e}")
        
        log.info(f"ðŸ”¨ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        start_total = time.time()
        
        # Step 1: Feature ë²¡í„° ì¶”ì¶œ ë° ì •ê·œí™”
        log.info(f"  Step 1/5: Feature ì •ê·œí™”...")
        start = time.time()
        
        feature_matrix = self.kb_df[available_features].values.astype(np.float32)
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)
        
        if HAS_SKLEARN:
            self.scaler = StandardScaler()
            normalized_features = self.scaler.fit_transform(feature_matrix).astype(np.float32)
        else:
            self.scaler = None
            normalized_features = feature_matrix
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # âœ… Step 2: SHAP One-Hot ì¸ì½”ë”© (ë²¡í„°í™” ë²„ì „)
        log.info(f"  Step 2/5: SHAP One-Hot ì¸ì½”ë”© (ë²¡í„°í™”)...")
        start = time.time()
        
        self.shap_feature_vocab = NETWORK_SECURITY_FEATURES
        shap_to_idx = {feat: i for i, feat in enumerate(self.shap_feature_vocab)}
        
        # ë¹ˆ í–‰ë ¬ ìƒì„±
        shap_onehot_matrix = np.zeros((len(self.kb_df), 22), dtype=np.float32)
        
        # ê° SHAP Top-K ì»¬ëŸ¼ì„ ë²¡í„°í™” ì²˜ë¦¬
        for rank in range(1, top_k + 1):
            feat_col = f"shap_top{rank}_feature"
            
            if feat_col not in self.kb_df.columns:
                continue
            
            # ì „ì²´ ì»¬ëŸ¼ì„ í•œë²ˆì— ê°€ì ¸ì˜´
            feat_series = self.kb_df[feat_col].astype(str)
            
            # ìœ íš¨í•œ ê°’ë§Œ í•„í„°ë§
            valid_mask = ~feat_series.isin(["", "nan", "None", "NaN"])
            
            # ê° Featureì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í–‰ ì°¾ê¸°
            for feat_name, feat_idx in shap_to_idx.items():
                # í•´ë‹¹ Featureì™€ ì¼ì¹˜í•˜ëŠ” í–‰ì˜ ì¸ë±ìŠ¤
                match_mask = valid_mask & (feat_series == feat_name)
                matching_rows = self.kb_df.index[match_mask].to_numpy()
                
                # One-Hot ì„¤ì •
                if len(matching_rows) > 0:
                    shap_onehot_matrix[matching_rows, feat_idx] = 1.0
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 3: ë³µí•© ë²¡í„° ìƒì„±
        log.info(f"  Step 3/5: ë³µí•© ë²¡í„° ìƒì„±...")
        start = time.time()
        
        composite_vectors = np.hstack([normalized_features, shap_onehot_matrix]).astype(np.float32)
        
        log.info(f"    â”œâ”€ Feature ì°¨ì›: {normalized_features.shape[1]}")
        log.info(f"    â”œâ”€ SHAP ì°¨ì›: {shap_onehot_matrix.shape[1]} (22ê°œ ê³ ì •)")
        log.info(f"    â””â”€ ì´ ì°¨ì›: {composite_vectors.shape[1]}")
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # âœ… Step 4: ë©”íƒ€ë°ì´í„° êµ¬ì¶• (ë²¡í„°í™”)
        log.info(f"  Step 4/5: ë©”íƒ€ë°ì´í„° êµ¬ì¶•...")
        start = time.time()
        
        self.metadata = []
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ (ë²¡í„°í™”)
        meta_cols = ["A_ip", "B_ip", "label", "case_id"]
        existing_meta_cols = [c for c in meta_cols if c in self.kb_df.columns]
        
        # SHAP ì»¬ëŸ¼ë“¤
        shap_cols = [f"shap_top{i}_feature" for i in range(1, top_k + 1) 
                    if f"shap_top{i}_feature" in self.kb_df.columns]
        
        for idx in range(len(self.kb_df)):
            row = self.kb_df.iloc[idx]
            
            meta = {
                "idx": idx,
                "A_ip": str(row.get("A_ip", "")),
                "B_ip": str(row.get("B_ip", "")),
                "label": str(row.get("label", "")),
                "case_id": str(row.get("case_id", "")),
                "shap_features": {}
            }
            
            # SHAP features ì¶”ì¶œ
            for rank, col in enumerate(shap_cols, 1):
                feat_name = str(row[col])
                if feat_name and feat_name not in ("", "nan", "None"):
                    if feat_name in shap_to_idx:
                        meta["shap_features"][feat_name] = rank
            
            self.metadata.append(meta)
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 5: FAISS ì¸ë±ìŠ¤ ìƒì„±
        log.info(f"  Step 5/5: FAISS ì¸ë±ìŠ¤ ìƒì„±...")
        start = time.time()
        
        dimension = composite_vectors.shape[1]
        self.feature_columns = available_features
        
        self.faiss_index = self._create_faiss_index(dimension)
        
        log.info(f"    âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # IVF Train
        if self.index_type == "IVF":
            log.info(f"    ðŸ”§ IVF í•™ìŠµ ì‹œìž‘ (GPU={'ON' if self.use_gpu else 'OFF'})...")
            start = time.time()
            
            self.faiss_index.train(composite_vectors)
            
            elapsed = time.time() - start
            log.info(f"    âœ… IVF í•™ìŠµ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        
        # ë²¡í„° ì¶”ê°€
        log.info(f"    ðŸ”§ ë²¡í„° ì¶”ê°€ ì‹œìž‘ ({len(composite_vectors)}ê°œ)...")
        start = time.time()
        
        self.faiss_index.add(composite_vectors)
        
        elapsed = time.time() - start
        vec_per_sec = len(composite_vectors) / elapsed if elapsed > 0 else 0
        log.info(f"    âœ… ë²¡í„° ì¶”ê°€ ì™„ë£Œ ({elapsed:.2f}ì´ˆ, {vec_per_sec:.0f} vec/s)")
        
        self.faiss_built = True
        
        # ìºì‹œ ì €ìž¥
        try:
            self._save_to_cache(cache_paths, composite_vectors)
            log.info(f"    ðŸ’¾ ìºì‹œ ì €ìž¥ ì™„ë£Œ")
        except Exception as e:
            log.warning(f"    âš ï¸ ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")
        
        total_elapsed = time.time() - start_total
        log.info(f"\nâœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.metadata)}ê°œ ì¼€ì´ìŠ¤ ({total_elapsed:.2f}ì´ˆ)")
        
        return True

    def _create_faiss_index(self, dimension: int):
        """FAISS ì¸ë±ìŠ¤ ìƒì„± (GPU ì „ì†¡ í¬í•¨)"""
        # CPU ì¸ë±ìŠ¤ ìƒì„±
        if self.index_type == "Flat":
            index = faiss.IndexFlatL2(dimension)
        elif self.index_type == "IVF":
            quantizer = faiss.IndexFlatL2(dimension)
            n_clusters = min(self.n_clusters, max(1, len(self.metadata) // 10))
            index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)
        elif self.index_type == "HNSW":
            index = faiss.IndexHNSWFlat(dimension, 32)
            index.hnsw.efConstruction = 40
        else:
            index = faiss.IndexFlatL2(dimension)
        
        # GPU ì „ì†¡
        if self.use_gpu:
            try:
                # GPU ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìž¬ì‚¬ìš©)
                if self.gpu_resources is None:
                    self.gpu_resources = faiss.StandardGpuResources()
                    # GPU ë©”ëª¨ë¦¬ ì„¤ì • (ë” ë§Žì´ í• ë‹¹)
                    self.gpu_resources.setTempMemory(1024 * 1024 * 1024)  # 1GB
                
                # GPUë¡œ ì „ì†¡
                index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, index)
                log.info(f"      âœ… GPUë¡œ ì¸ë±ìŠ¤ ì „ì†¡ ì„±ê³µ (Device 0)")
                
            except Exception as e:
                log.error(f"      âŒ GPU ì „ì†¡ ì‹¤íŒ¨: {e}")
                log.error(f"      âš ï¸ CPU ëª¨ë“œë¡œ fallbackí•©ë‹ˆë‹¤")
                self.use_gpu = False  # GPU í”Œëž˜ê·¸ ë”
        
        return index

    def search_similar_cases(self, query_features: np.ndarray, 
                            query_shap_features: Dict[str, int],
                            k: int = 100) -> List[Tuple[int, float]]:
        """
        FAISSë¡œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query_features: ì¿¼ë¦¬ feature ë²¡í„° (ì •ê·œí™” ì „)
            query_shap_features: ì¿¼ë¦¬ SHAP features {name: rank}
            k: ë°˜í™˜í•  í›„ë³´ ìˆ˜
            
        Returns:
            [(kb_index, distance), ...]
        """
        if not self.faiss_built:
            log.warning("FAISS ì¸ë±ìŠ¤ ë¯¸êµ¬ì¶•")
            return []
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        query_features = np.nan_to_num(query_features, nan=0.0).astype(np.float32)
        if self.scaler is not None:
            normalized_query = self.scaler.transform(query_features.reshape(1, -1)).astype(np.float32)
        else:
            normalized_query = query_features.reshape(1, -1).astype(np.float32)
        
        # SHAP One-Hot (22ê°œ ê³ ì •)
        query_shap_onehot = np.zeros((1, 22), dtype=np.float32)
        shap_to_idx = {feat: i for i, feat in enumerate(self.shap_feature_vocab)}
        
        for feat_name in query_shap_features.keys():
            if feat_name in shap_to_idx:
                feat_idx = shap_to_idx[feat_name]
                query_shap_onehot[0, feat_idx] = 1.0
        
        # ë³µí•© ë²¡í„°
        query_composite = np.hstack([normalized_query, query_shap_onehot]).astype(np.float32)
        
        # ê²€ìƒ‰
        if self.index_type == "IVF":
            # nprobe ì„¤ì • (ê²€ìƒ‰ ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
            if hasattr(self.faiss_index, 'nprobe'):
                self.faiss_index.nprobe = min(10, self.n_clusters)
        
        distances, indices = self.faiss_index.search(query_composite, k)
        
        results = []
        for i in range(len(indices[0])):
            idx = int(indices[0][i])
            dist = float(distances[0][i])
            if idx >= 0:
                results.append((idx, dist))
        
        return results

    def get_labeled_cases(self, labels: List[str] = None) -> pd.DataFrame:
        """
        íŠ¹ì • ë¼ë²¨ì˜ ì‚¬ë¡€ë§Œ ë°˜í™˜ (backward compatible)
        """
        if not self.is_loaded or self.kb_df is None:
            return pd.DataFrame()
        if labels is None:
            return self.kb_df.copy()
        return self.kb_df[self.kb_df["label"].isin(labels)].copy()

    def get_stats(self) -> Dict[str, any]:
        """Knowledge Base í†µê³„ ë°˜í™˜"""
        if not self.is_loaded or self.kb_df is None:
            return {}
        
        stats = {
            "total": len(self.kb_df),
            "labels": self.kb_df["label"].value_counts().to_dict() if "label" in self.kb_df.columns else {},
            "files_loaded": len(glob.glob(os.path.join(self.train_cases_dir, "*.csv"))),
            "faiss_enabled": self.faiss_built,
            "gpu_enabled": self.use_gpu,
        }
        
        if self.faiss_built:
            stats["faiss_index_type"] = self.index_type
            stats["faiss_dimension"] = len(self.feature_columns) + 22  # âœ… 22ê°œ ê³ ì •
            stats["feature_dim"] = len(self.feature_columns)
            stats["shap_vocab_dim"] = 22  # âœ… 22ê°œ ê³ ì •
        
        return stats

    def export_as_feedback_corpus(self, out_dir: str = "./feedback_cases") -> str:
        """
        Knowledge Baseë¥¼ í”¼ë“œë°± ì½”í¼ìŠ¤ í˜•ì‹ìœ¼ë¡œ Export (backward compatible)
        """
        if not self.is_loaded or self.kb_df is None:
            log.error("Knowledge Base ë¯¸ë¡œë“œ")
            return ""

        os.makedirs(out_dir, exist_ok=True)
        export_df = self.kb_df.copy()
        
        for col, default in [
            ("case_id", ""),
            ("feedback_label", ""),
            ("feedback_confidence", ""),
            ("feedback_reason", ""),
            ("reviewed", True),
            ("needs_review", False),
            ("review_date", ""),
        ]:
            if col not in export_df.columns:
                if col == "reviewed":
                    export_df[col] = True
                elif col == "case_id" and "case_id" not in export_df.columns:
                    export_df[col] = [f"KB_{i:06d}" for i in range(len(export_df))]
                elif col == "feedback_label":
                    export_df[col] = export_df.get("label", "")
                elif col == "feedback_confidence":
                    export_df[col] = 5
                elif col == "feedback_reason":
                    export_df[col] = "(Train Knowledge Base)"
                else:
                    export_df[col] = default

        out_path = os.path.join(out_dir, "Knowledge_Base_Corpus.csv")
        export_df.to_csv(out_path, index=False, encoding="utf-8")
        log.info(f"Knowledge Base Export: {out_path} ({len(export_df)} rows)")
        return out_path

    # ========================================
    # ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    # ========================================
    
    def _compute_data_hash(self, kb_df: pd.DataFrame, feature_cols: List[str]) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        key_parts = [
            f"rows:{len(kb_df)}",
            f"cols:{len(feature_cols)}",
            f"features:{','.join(sorted(feature_cols))}",
        ]
        if len(kb_df) > 0:
            sample_size = min(100, len(kb_df))
            sample_idx = np.linspace(0, len(kb_df)-1, sample_size, dtype=int)
            sample_data = kb_df.iloc[sample_idx][feature_cols].values
            data_hash = hashlib.md5(sample_data.tobytes()).hexdigest()[:16]
            key_parts.append(f"data:{data_hash}")
        return hashlib.md5("_".join(key_parts).encode()).hexdigest()
    
    def _get_cache_paths(self, data_hash: str) -> Dict[str, str]:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
        prefix = os.path.join(self.cache_dir, f"kb_{data_hash}")
        return {
            "index": f"{prefix}_index.faiss",
            "scaler": f"{prefix}_scaler.pkl",
            "metadata": f"{prefix}_metadata.pkl",
            "config": f"{prefix}_config.pkl",
        }
    
    def _save_to_cache(self, cache_paths: Dict[str, str], vectors: np.ndarray):
        """ìºì‹œ ì €ìž¥"""
        # GPU ì¸ë±ìŠ¤ëŠ” CPUë¡œ ë³€í™˜ í›„ ì €ìž¥
        if self.use_gpu:
            cpu_index = faiss.index_gpu_to_cpu(self.faiss_index)
            faiss.write_index(cpu_index, cache_paths["index"])
        else:
            faiss.write_index(self.faiss_index, cache_paths["index"])
        
        if self.scaler is not None:
            with open(cache_paths["scaler"], "wb") as f:
                pickle.dump(self.scaler, f)
        
        with open(cache_paths["metadata"], "wb") as f:
            pickle.dump(self.metadata, f)
        
        config = {
            "feature_columns": self.feature_columns,
            "shap_feature_vocab": self.shap_feature_vocab,
            "index_type": self.index_type,
            "n_clusters": self.n_clusters,
        }
        with open(cache_paths["config"], "wb") as f:
            pickle.dump(config, f)
    
    def _load_from_cache(self, cache_paths: Dict[str, str]) -> bool:
        """ìºì‹œ ë¡œë“œ"""
        with open(cache_paths["config"], "rb") as f:
            config = pickle.load(f)
        
        self.feature_columns = config["feature_columns"]
        self.shap_feature_vocab = config.get("shap_feature_vocab", NETWORK_SECURITY_FEATURES)
        
        self.faiss_index = faiss.read_index(cache_paths["index"])
        
        # GPUë¡œ ì „ì†¡
        if self.use_gpu:
            try:
                if self.gpu_resources is None:
                    self.gpu_resources = faiss.StandardGpuResources()
                    self.gpu_resources.setTempMemory(1024 * 1024 * 1024)
                
                self.faiss_index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, self.faiss_index)
                log.info("âœ… ìºì‹œëœ ì¸ë±ìŠ¤ë¥¼ GPUë¡œ ì „ì†¡ ì™„ë£Œ")
            except Exception as e:
                log.warning(f"âš ï¸ GPU ì „ì†¡ ì‹¤íŒ¨, CPUë¡œ ì‚¬ìš©: {e}")
                self.use_gpu = False
        
        if os.path.exists(cache_paths["scaler"]):
            with open(cache_paths["scaler"], "rb") as f:
                self.scaler = pickle.load(f)
        
        with open(cache_paths["metadata"], "rb") as f:
            self.metadata = pickle.load(f)
        
        self.faiss_built = True
        return True

    def __len__(self) -> int:
        return len(self.kb_df) if self.is_loaded and self.kb_df is not None else 0

    def __repr__(self) -> str:
        if not self.is_loaded:
            return "KnowledgeBase(not loaded)"
        stats = self.get_stats()
        gpu_info = f", GPU={'ON' if self.use_gpu else 'OFF'}" if self.faiss_built else ""
        faiss_info = f", FAISS({self.index_type}){gpu_info}" if self.faiss_built else ""
        return f"KnowledgeBase(total={stats.get('total', 0)}, labels={stats.get('labels', {})}{faiss_info})"