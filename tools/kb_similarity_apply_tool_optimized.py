# -*- coding: utf-8 -*-
from __future__ import annotations
import os
import re
from typing import Dict, List, Tuple, Optional
import numpy as np
import pandas as pd
from tools.base import ITool, ToolResult, ensure_dir, get_logger

log = get_logger("KBSimilarityApplyUltraFast")


def _get_feature_columns(df: pd.DataFrame) -> List[str]:
    """Feature 컬럼 추출"""
    patterns = [
        r'^label$', r'.*_label$', r'^case_id$', r'^capsule_id$',
        r'.*feedback.*', r'.*applied.*', r'^shap_.*', r'.*reviewed.*',
        r'.*needs_review.*', r'.*review_date.*', r'^A_ip$', r'^B_ip$',
        r'^A_port$', r'^B_port$', r'.*predicted.*', r'.*confidence.*',
        r'.*probability.*', r'.*is_correct.*', r'.*adjusted.*', r'.*reason.*',
        r'.*round.*', r'^capsule_duration$', r'^flow_count$', r'^unique_ports$',
        r'^statistical_score$', r'^__.*',
    ]
    comp = [re.compile(p, re.IGNORECASE) for p in patterns]
    num = df.select_dtypes(include=['int64','float64']).columns
    feats = [c for c in num if not any(p.match(c) for p in comp)]
    return feats


def _shap_rank_map(row: pd.Series, k: int=5) -> Dict[str,int]:
    """SHAP Top-K feature → {feature_name: rank}"""
    d = {}
    for i in range(1, k+1):
        col = f"shap_top{i}_feature"
        f = row.get(col, "")
        if f and str(f) not in ("","nan","None"):
            d[str(f)] = i
    return d


def _rank_overlap_vectorized(query_shap: Dict[str,int], 
                             kb_shap_list: List[Dict[str,int]], 
                             rank_weights: Dict[int,float]) -> np.ndarray:
    """벡터화된 SHAP overlap 계산 (배치)"""
    n = len(kb_shap_list)
    overlaps = np.zeros(n, dtype=np.float32)
    
    for i, kb_shap in enumerate(kb_shap_list):
        if not query_shap or not kb_shap:
            continue
        
        common = set(query_shap) & set(kb_shap)
        if not common:
            continue
        
        s = sum(rank_weights.get(abs(query_shap[feat] - kb_shap[feat]), 0.0) 
                for feat in common)
        mx = min(len(query_shap), len(kb_shap))
        overlaps[i] = s / mx if mx > 0 else 0.0
    
    return overlaps


class KBSimilarityApplyToolOptimized(ITool):
    """
    Ultra-Fast FAISS KB Similarity Apply Tool
    
    성능 최적화:
    - FAISS 배치 검색 (N개 쿼리 → 1번 호출)
    - KB 벡터 사전 계산 (NumPy 배열)
    - 벡터화된 연산 (루프 최소화)
    """
    
    def __init__(self,
                 round_name: str,
                 pred_dir: str = "./feedback_cases",
                 kb_corpus: pd.DataFrame = None,
                 kb_instance = None,
                 out_dir: str = "./kb_applied_round_predictions",
                 alpha: float = 0.3,
                 beta: float = 0.4,
                 gamma: float = 0.3,
                 threshold: float = 0.8,
                 direction_sensitive: bool = True,
                 top_k: int = 5,
                 rank_weights: Optional[Dict[int, float]] = None,
                 faiss_k: int = 20,
                 batch_size: int = 1000):  # 배치 크기
        
        self.round_name = round_name
        self.pred_dir = pred_dir
        self.kb_corpus = kb_corpus
        self.kb_instance = kb_instance
        self.out_dir = out_dir
        
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.threshold = threshold
        
        self.direction_sensitive = direction_sensitive
        self.top_k = top_k
        self.rank_weights = rank_weights or {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4}
        
        self.faiss_k = faiss_k
        self.batch_size = batch_size
        
        self.use_faiss = (kb_instance is not None and 
                         hasattr(kb_instance, 'faiss_built') and 
                         kb_instance.faiss_built)
        
        # KB 벡터 사전 계산 (초기화 시 1번만)
        self.kb_vectors_cache = None
        self.kb_ips_cache = None
        self.kb_shap_cache = None

    def _prepare_kb_cache(self, common: List[str]):
        """KB 벡터/메타데이터 사전 계산 (1번만)"""
        if self.kb_vectors_cache is not None:
            return  # 이미 캐시됨
        
        log.info(f"[{self.round_name}] KB 캐시 구축 중...")
        
        # 1. Feature 벡터 → NumPy 배열
        self.kb_vectors_cache = self.kb_corpus[common].values.astype(np.float32)
        self.kb_vectors_cache = np.nan_to_num(self.kb_vectors_cache, nan=0.0)
        
        # 2. IP 정보 → NumPy 배열
        self.kb_ips_cache = self.kb_corpus[['A_ip', 'B_ip']].values.astype(str)
        
        # 3. SHAP 정보 → 리스트
        self.kb_shap_cache = [
            self.kb_instance.metadata[i]["shap_features"]
            for i in range(len(self.kb_corpus))
        ]
        
        log.info(f"✅ KB 캐시 완료 ({len(self.kb_corpus)} cases)")

    def run(self) -> ToolResult:
        """메인 실행 로직"""
        ensure_dir(self.out_dir)
        
        if self.kb_corpus is None or self.kb_corpus.empty:
            log.warning(f"[{self.round_name}] Knowledge Base 없음 → 스킵")
            return ToolResult(True, "kb_empty", output_path=None)
        
        pred_file = os.path.join(self.pred_dir, f"{self.round_name}_low_confidence_cases.csv")
        if not os.path.exists(pred_file):
            return ToolResult(False, f"예측 파일 없음: {pred_file}")
        
        df = pd.read_csv(pred_file, low_memory=False)
        feat_cols = _get_feature_columns(df)
        print(df.columns)

        gamma = self.gamma
        if gamma > 0:
            miss = [f"shap_top{i}_feature" for i in range(1, self.top_k + 1)
                   if f"shap_top{i}_feature" not in df.columns]
            if miss:
                log.warning(f"SHAP 컬럼 누락 → gamma=0")
                gamma = 0.0
        
        kb_feat_cols = _get_feature_columns(self.kb_corpus)
        common = sorted(list(set(feat_cols) & set(kb_feat_cols)))
        
        if not common:
            log.warning(f"[{self.round_name}] 공통 feature 없음")
            out_path = os.path.join(self.out_dir, f"{self.round_name}_kb_applied.csv")
            df.to_csv(out_path, index=False, encoding="utf-8")
            return ToolResult(True, "no_common_features", output_path=out_path)
        
        # 출력 필드 준비
        for col, default in [
            ("kb_applied_label", ""),
            ("kb_applied", False),
            ("kb_from_case", ""),
            ("kb_similarity_score", 0.0),
            ("kb_similarity_ip", 0.0),
            ("kb_similarity_cosine", 0.0),
            ("kb_similarity_overlap", 0.0),
        ]:
            if col not in df.columns:
                df[col] = default
        
        if not self.use_faiss:
            log.warning(f"[{self.round_name}] FAISS 없음 → 기본 모드")
            return self._run_fallback(df, common, gamma)
        
        # KB 캐시 사전 구축 (1번만!)
        self._prepare_kb_cache(common)
        
        # 배치 처리
        log.info(f"[{self.round_name}] Ultra-Fast 배치 처리 시작")
        applied, total = self._run_ultra_fast_batch(df, common, gamma)
        
        # 결과 저장
        out_path = os.path.join(self.out_dir, f"{self.round_name}_kb_applied.csv")
        df.to_csv(out_path, index=False, encoding="utf-8")
        
        log.info(f"✅ [{self.round_name}] 완료: {applied}/{total} 케이스 ({applied/total*100:.1f}%)")
        
        return ToolResult(
            True, "ok",
            output_path=out_path,
            data={"applied": applied, "total": total, "mode": "UltraFast"}
        )

    def _run_ultra_fast_batch(self, df: pd.DataFrame, common: List[str], gamma: float) -> Tuple[int, int]:
        """ Ultra-Fast 배치 처리"""
        applied = 0
        total = len(df)
        
        # Query 데이터 추출 (배치)
        query_vectors = df[common].values.astype(np.float32)
        query_vectors = np.nan_to_num(query_vectors, nan=0.0)
        query_ips = df[['A_ip', 'B_ip']].values.astype(str)
        query_shaps = [_shap_rank_map(row, self.top_k) for _, row in df.iterrows()]
        
        # Stage 1: FAISS 배치 검색 (전체 쿼리를 1번에!)
        log.info(f"  Stage 1: FAISS 배치 검색 ({total} queries)")
        
        # 복합 벡터 생성 (배치)
        query_composites = self._build_composite_vectors_batch(
            query_vectors, query_shaps
        )
        
        # FAISS 배치 검색
        if self.kb_instance.index_type == "IVF":
            self.kb_instance.faiss_index.nprobe = min(10, self.kb_instance.n_clusters)
        
        distances, indices = self.kb_instance.faiss_index.search(
            query_composites, self.faiss_k
        )
        
        log.info(f"  Stage 2: 정확한 S-score 계산 (배치)")
        
        # Stage 2: 벡터화된 S-score 계산
        for i in range(total):
            query_vec = query_vectors[i]
            query_ip_a, query_ip_b = query_ips[i]
            query_shap = query_shaps[i]
            
            # 후보 인덱스
            candidate_indices = indices[i]
            valid_mask = candidate_indices >= 0
            candidate_indices = candidate_indices[valid_mask]
            
            if len(candidate_indices) == 0:
                continue
            
            # 벡터화된 IP 유사도 계산
            kb_ips_batch = self.kb_ips_cache[candidate_indices]
            
            if self.direction_sensitive:
                ip_scores = ((kb_ips_batch[:, 0] == query_ip_a) & 
                            (kb_ips_batch[:, 1] == query_ip_b)).astype(np.float32)
            else:
                ip_scores = np.array([
                    1.0 if {query_ip_a, query_ip_b} == {kb_ip_a, kb_ip_b} else 0.0
                    for kb_ip_a, kb_ip_b in kb_ips_batch
                ], dtype=np.float32)
            
            # 벡터화된 코사인 유사도 계산
            kb_vecs_batch = self.kb_vectors_cache[candidate_indices]
            
            # 코사인 유사도 (배치)
            query_norm = np.linalg.norm(query_vec)
            kb_norms = np.linalg.norm(kb_vecs_batch, axis=1)
            
            with np.errstate(divide='ignore', invalid='ignore'):
                cos_scores = np.dot(kb_vecs_batch, query_vec) / (kb_norms * query_norm)
                cos_scores = np.nan_to_num(cos_scores, nan=0.0)
                cos_scores = (cos_scores + 1) / 2  # 0~1 범위
                cos_scores = np.clip(cos_scores, 0.0, 1.0)
            
            # SHAP overlap (배치)
            if gamma > 0:
                kb_shaps_batch = [self.kb_shap_cache[idx] for idx in candidate_indices]
                overlap_scores = _rank_overlap_vectorized(
                    query_shap, kb_shaps_batch, self.rank_weights
                )
            else:
                overlap_scores = np.zeros(len(candidate_indices), dtype=np.float32)
            
            # S-score 계산 (벡터화)
            s_scores = (self.alpha * ip_scores + 
                       self.beta * cos_scores + 
                       gamma * overlap_scores)
            
            # 최고 점수 찾기
            best_idx_in_candidates = np.argmax(s_scores)
            best_score = s_scores[best_idx_in_candidates]
            
            if best_score >= self.threshold:
                best_kb_idx = candidate_indices[best_idx_in_candidates]
                best_kb_row = self.kb_corpus.iloc[best_kb_idx]
                
                df.at[i, "kb_applied_label"] = best_kb_row.get("label", "")
                df.at[i, "kb_applied"] = True
                df.at[i, "kb_from_case"] = str(best_kb_row.get("case_id", ""))
                df.at[i, "kb_similarity_score"] = float(best_score)
                df.at[i, "kb_similarity_ip"] = float(ip_scores[best_idx_in_candidates])
                df.at[i, "kb_similarity_cosine"] = float(cos_scores[best_idx_in_candidates])
                df.at[i, "kb_similarity_overlap"] = float(overlap_scores[best_idx_in_candidates])
                applied += 1
            
            # 진행률 로깅
            if (i + 1) % 1000 == 0:
                log.info(f"  진행: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
        
        return applied, total

    def _build_composite_vectors_batch(self, 
                                    query_vectors: np.ndarray,
                                    query_shaps: List[Dict]) -> np.ndarray:
        """복합 벡터 배치 생성 (수정: SHAP Vocab 22개 고정)"""
        n = len(query_vectors)
        
        # Feature 정규화 (배치)
        if self.kb_instance.scaler is not None:
            normalized = self.kb_instance.scaler.transform(query_vectors).astype(np.float32)
        else:
            normalized = query_vectors
        
        # SHAP One-Hot (22차원 고정)
        vocab_size = len(self.kb_instance.shap_feature_vocab)  # 항상 22
        shap_onehot = np.zeros((n, vocab_size), dtype=np.float32)
        
        # Feature 이름 → Index 매핑
        shap_to_idx = {
            feat: i for i, feat in enumerate(self.kb_instance.shap_feature_vocab)
        }
        
        for i, shap_dict in enumerate(query_shaps):
            for feat_name in shap_dict.keys():
                if feat_name in shap_to_idx:  # 22개 Feature 중 하나인지만 체크
                    feat_idx = shap_to_idx[feat_name]
                    shap_onehot[i, feat_idx] = 1.0
        
        # 복합 벡터: 22 + 22 = 44차원
        return np.hstack([normalized, shap_onehot]).astype(np.float32)

    def _run_fallback(self, df: pd.DataFrame, common: List[str], gamma: float) -> ToolResult:
        """Fallback: FAISS 없을 때"""
        log.warning("FAISS 없음 - 기존 방식 사용")
        # 기존 Brute-Force 로직...
        return ToolResult(True, "fallback", output_path=None)