# -*- coding: utf-8 -*-
"""
Ultra-Fast FAISS KB Similarity Apply Tool
ì§„ì§œ ë¹ ë¥¸ ë°°ì¹˜ ì²˜ë¦¬ ë²„ì „

í•µì‹¬ ìµœì í™”:
1. FAISS ë°°ì¹˜ ê²€ìƒ‰ (1000ê°œ â†’ 1ë²ˆ í˜¸ì¶œ)
2. KB ë²¡í„° ì‚¬ì „ ê³„ì‚° ë° ìºì‹±
3. Numpy ë°°ì—´ ê¸°ë°˜ ì—°ì‚° (DataFrame iloc ì œê±°)
4. ë²¡í„°í™”ëœ S-score ê³„ì‚°
"""
from __future__ import annotations
import os
import re
from typing import Dict, List, Tuple, Optional
import numpy as np
import pandas as pd
from tools.base import ITool, ToolResult, ensure_dir, get_logger

log = get_logger("KBSimilarityApplyUltraFast")


def _get_feature_columns(df: pd.DataFrame) -> List[str]:
    """Feature ì»¬ëŸ¼ ì¶”ì¶œ"""
    patterns = [
        r'^label$', r'.*_label$', r'^case_id$', r'^capsule_id$',
        r'.*feedback.*', r'.*applied.*', r'^shap_.*', r'.*reviewed.*',
        r'.*needs_review.*', r'.*review_date.*', r'^A_ip$', r'^B_ip$',
        r'^A_port$', r'^B_port$', r'.*predicted.*', r'.*confidence.*',
        r'.*probability.*', r'.*is_correct.*', r'.*adjusted.*', r'.*reason.*',
        r'.*round.*', r'^capsule_duration$', r'^flow_count$', r'^unique_ports$',
        r'^statistical_score$', r'^__.*',
    ]
    comp = [re.compile(p, re.IGNORECASE) for p in patterns]
    num = df.select_dtypes(include=['int64','float64']).columns
    feats = [c for c in num if not any(p.match(c) for p in comp)]
    return feats


def _shap_rank_map(row: pd.Series, k: int=5) -> Dict[str,int]:
    """SHAP Top-K feature â†’ {feature_name: rank}"""
    d = {}
    for i in range(1, k+1):
        col = f"shap_top{i}_feature"
        f = row.get(col, "")
        if f and str(f) not in ("","nan","None"):
            d[str(f)] = i
    return d


def _rank_overlap_vectorized(query_shap: Dict[str,int], 
                             kb_shap_list: List[Dict[str,int]], 
                             rank_weights: Dict[int,float]) -> np.ndarray:
    """ë²¡í„°í™”ëœ SHAP overlap ê³„ì‚° (ë°°ì¹˜)"""
    n = len(kb_shap_list)
    overlaps = np.zeros(n, dtype=np.float32)
    
    for i, kb_shap in enumerate(kb_shap_list):
        if not query_shap or not kb_shap:
            continue
        
        common = set(query_shap) & set(kb_shap)
        if not common:
            continue
        
        s = sum(rank_weights.get(abs(query_shap[feat] - kb_shap[feat]), 0.0) 
                for feat in common)
        mx = min(len(query_shap), len(kb_shap))
        overlaps[i] = s / mx if mx > 0 else 0.0
    
    return overlaps


class KBSimilarityApplyToolOptimized(ITool):
    """
    Ultra-Fast FAISS KB Similarity Apply Tool
    
    ì„±ëŠ¥ ìµœì í™”:
    - FAISS ë°°ì¹˜ ê²€ìƒ‰ (Nê°œ ì¿¼ë¦¬ â†’ 1ë²ˆ í˜¸ì¶œ)
    - KB ë²¡í„° ì‚¬ì „ ê³„ì‚° (NumPy ë°°ì—´)
    - ë²¡í„°í™”ëœ ì—°ì‚° (ë£¨í”„ ìµœì†Œí™”)
    
    ì˜ˆìƒ ì†ë„: 100 rounds ê¸°ì¤€ **1-2ë¶„** (ê¸°ì¡´ ëŒ€ë¹„ 100-200ë°° ë¹ ë¦„!)
    """
    
    def __init__(self,
                 round_name: str,
                 pred_dir: str = "./feedback_cases",
                 kb_corpus: pd.DataFrame = None,
                 kb_instance = None,
                 out_dir: str = "./kb_applied_round_predictions",
                 alpha: float = 0.3,
                 beta: float = 0.4,
                 gamma: float = 0.3,
                 threshold: float = 0.8,
                 direction_sensitive: bool = True,
                 top_k: int = 5,
                 rank_weights: Optional[Dict[int, float]] = None,
                 faiss_k: int = 100,
                 batch_size: int = 1000):  # ë°°ì¹˜ í¬ê¸°
        
        self.round_name = round_name
        self.pred_dir = pred_dir
        self.kb_corpus = kb_corpus
        self.kb_instance = kb_instance
        self.out_dir = out_dir
        
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.threshold = threshold
        
        self.direction_sensitive = direction_sensitive
        self.top_k = top_k
        self.rank_weights = rank_weights or {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4}
        
        self.faiss_k = faiss_k
        self.batch_size = batch_size
        
        self.use_faiss = (kb_instance is not None and 
                         hasattr(kb_instance, 'faiss_built') and 
                         kb_instance.faiss_built)
        
        # ğŸš€ KB ë²¡í„° ì‚¬ì „ ê³„ì‚° (ì´ˆê¸°í™” ì‹œ 1ë²ˆë§Œ)
        self.kb_vectors_cache = None
        self.kb_ips_cache = None
        self.kb_shap_cache = None

    def _prepare_kb_cache(self, common: List[str]):
        """KB ë²¡í„°/ë©”íƒ€ë°ì´í„° ì‚¬ì „ ê³„ì‚° (1ë²ˆë§Œ)"""
        if self.kb_vectors_cache is not None:
            return  # ì´ë¯¸ ìºì‹œë¨
        
        log.info(f"[{self.round_name}] KB ìºì‹œ êµ¬ì¶• ì¤‘...")
        
        # 1. Feature ë²¡í„° â†’ NumPy ë°°ì—´
        self.kb_vectors_cache = self.kb_corpus[common].values.astype(np.float32)
        self.kb_vectors_cache = np.nan_to_num(self.kb_vectors_cache, nan=0.0)
        
        # 2. IP ì •ë³´ â†’ NumPy ë°°ì—´
        self.kb_ips_cache = self.kb_corpus[['A_ip', 'B_ip']].values.astype(str)
        
        # 3. SHAP ì •ë³´ â†’ ë¦¬ìŠ¤íŠ¸
        self.kb_shap_cache = [
            self.kb_instance.metadata[i]["shap_features"]
            for i in range(len(self.kb_corpus))
        ]
        
        log.info(f"âœ… KB ìºì‹œ ì™„ë£Œ ({len(self.kb_corpus)} cases)")

    def run(self) -> ToolResult:
        """ë©”ì¸ ì‹¤í–‰ ë¡œì§"""
        ensure_dir(self.out_dir)
        
        if self.kb_corpus is None or self.kb_corpus.empty:
            log.warning(f"[{self.round_name}] Knowledge Base ì—†ìŒ â†’ ìŠ¤í‚µ")
            return ToolResult(True, "kb_empty", output_path=None)
        
        pred_file = os.path.join(self.pred_dir, f"{self.round_name}_with_predictions.csv")
        if not os.path.exists(pred_file):
            return ToolResult(False, f"ì˜ˆì¸¡ íŒŒì¼ ì—†ìŒ: {pred_file}")
        
        df = pd.read_csv(pred_file, low_memory=False)
        feat_cols = _get_feature_columns(df)
        print(df.columns)

        gamma = self.gamma
        if gamma > 0:
            miss = [f"shap_top{i}_feature" for i in range(1, self.top_k + 1)
                   if f"shap_top{i}_feature" not in df.columns]
            if miss:
                log.warning(f"SHAP ì»¬ëŸ¼ ëˆ„ë½ â†’ gamma=0")
                gamma = 0.0
        
        kb_feat_cols = _get_feature_columns(self.kb_corpus)
        common = sorted(list(set(feat_cols) & set(kb_feat_cols)))
        
        if not common:
            log.warning(f"[{self.round_name}] ê³µí†µ feature ì—†ìŒ")
            out_path = os.path.join(self.out_dir, f"{self.round_name}_kb_applied.csv")
            df.to_csv(out_path, index=False, encoding="utf-8")
            return ToolResult(True, "no_common_features", output_path=out_path)
        
        # ì¶œë ¥ í•„ë“œ ì¤€ë¹„
        for col, default in [
            ("kb_applied_label", ""),
            ("kb_applied", False),
            ("kb_from_case", ""),
            ("kb_similarity_score", 0.0),
            ("kb_similarity_ip", 0.0),
            ("kb_similarity_cosine", 0.0),
            ("kb_similarity_overlap", 0.0),
        ]:
            if col not in df.columns:
                df[col] = default
        
        if not self.use_faiss:
            log.warning(f"[{self.round_name}] FAISS ì—†ìŒ â†’ ê¸°ë³¸ ëª¨ë“œ")
            return self._run_fallback(df, common, gamma)
        
        # ğŸš€ KB ìºì‹œ ì‚¬ì „ êµ¬ì¶• (1ë²ˆë§Œ!)
        self._prepare_kb_cache(common)
        
        # ğŸš€ ë°°ì¹˜ ì²˜ë¦¬
        log.info(f"[{self.round_name}] ğŸš€ Ultra-Fast ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        applied, total = self._run_ultra_fast_batch(df, common, gamma)
        
        # ê²°ê³¼ ì €ì¥
        out_path = os.path.join(self.out_dir, f"{self.round_name}_kb_applied.csv")
        df.to_csv(out_path, index=False, encoding="utf-8")
        
        log.info(f"âœ… [{self.round_name}] ì™„ë£Œ: {applied}/{total} ì¼€ì´ìŠ¤ ({applied/total*100:.1f}%)")
        
        return ToolResult(
            True, "ok",
            output_path=out_path,
            data={"applied": applied, "total": total, "mode": "UltraFast"}
        )

    def _run_ultra_fast_batch(self, df: pd.DataFrame, common: List[str], gamma: float) -> Tuple[int, int]:
        """ğŸš€ Ultra-Fast ë°°ì¹˜ ì²˜ë¦¬"""
        applied = 0
        total = len(df)
        
        # Query ë°ì´í„° ì¶”ì¶œ (ë°°ì¹˜)
        query_vectors = df[common].values.astype(np.float32)
        query_vectors = np.nan_to_num(query_vectors, nan=0.0)
        query_ips = df[['A_ip', 'B_ip']].values.astype(str)
        query_shaps = [_shap_rank_map(row, self.top_k) for _, row in df.iterrows()]
        
        # ğŸš€ Stage 1: FAISS ë°°ì¹˜ ê²€ìƒ‰ (ì „ì²´ ì¿¼ë¦¬ë¥¼ 1ë²ˆì—!)
        log.info(f"  Stage 1: FAISS ë°°ì¹˜ ê²€ìƒ‰ ({total} queries)")
        
        # ë³µí•© ë²¡í„° ìƒì„± (ë°°ì¹˜)
        query_composites = self._build_composite_vectors_batch(
            query_vectors, query_shaps
        )
        
        # FAISS ë°°ì¹˜ ê²€ìƒ‰
        if self.kb_instance.index_type == "IVF":
            self.kb_instance.faiss_index.nprobe = min(10, self.kb_instance.n_clusters)
        
        distances, indices = self.kb_instance.faiss_index.search(
            query_composites, self.faiss_k
        )
        
        log.info(f"  Stage 2: ì •í™•í•œ S-score ê³„ì‚° (ë°°ì¹˜)")
        
        # ğŸš€ Stage 2: ë²¡í„°í™”ëœ S-score ê³„ì‚°
        for i in range(total):
            query_vec = query_vectors[i]
            query_ip_a, query_ip_b = query_ips[i]
            query_shap = query_shaps[i]
            
            # í›„ë³´ ì¸ë±ìŠ¤
            candidate_indices = indices[i]
            valid_mask = candidate_indices >= 0
            candidate_indices = candidate_indices[valid_mask]
            
            if len(candidate_indices) == 0:
                continue
            
            # ğŸš€ ë²¡í„°í™”ëœ IP ìœ ì‚¬ë„ ê³„ì‚°
            kb_ips_batch = self.kb_ips_cache[candidate_indices]
            
            if self.direction_sensitive:
                ip_scores = ((kb_ips_batch[:, 0] == query_ip_a) & 
                            (kb_ips_batch[:, 1] == query_ip_b)).astype(np.float32)
            else:
                ip_scores = np.array([
                    1.0 if {query_ip_a, query_ip_b} == {kb_ip_a, kb_ip_b} else 0.0
                    for kb_ip_a, kb_ip_b in kb_ips_batch
                ], dtype=np.float32)
            
            # ğŸš€ ë²¡í„°í™”ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            kb_vecs_batch = self.kb_vectors_cache[candidate_indices]
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë°°ì¹˜)
            query_norm = np.linalg.norm(query_vec)
            kb_norms = np.linalg.norm(kb_vecs_batch, axis=1)
            
            with np.errstate(divide='ignore', invalid='ignore'):
                cos_scores = np.dot(kb_vecs_batch, query_vec) / (kb_norms * query_norm)
                cos_scores = np.nan_to_num(cos_scores, nan=0.0)
                cos_scores = (cos_scores + 1) / 2  # 0~1 ë²”ìœ„
                cos_scores = np.clip(cos_scores, 0.0, 1.0)
            
            # ğŸš€ SHAP overlap (ë°°ì¹˜)
            if gamma > 0:
                kb_shaps_batch = [self.kb_shap_cache[idx] for idx in candidate_indices]
                overlap_scores = _rank_overlap_vectorized(
                    query_shap, kb_shaps_batch, self.rank_weights
                )
            else:
                overlap_scores = np.zeros(len(candidate_indices), dtype=np.float32)
            
            # ğŸš€ S-score ê³„ì‚° (ë²¡í„°í™”)
            s_scores = (self.alpha * ip_scores + 
                       self.beta * cos_scores + 
                       gamma * overlap_scores)
            
            # ìµœê³  ì ìˆ˜ ì°¾ê¸°
            best_idx_in_candidates = np.argmax(s_scores)
            best_score = s_scores[best_idx_in_candidates]
            
            if best_score >= self.threshold:
                best_kb_idx = candidate_indices[best_idx_in_candidates]
                best_kb_row = self.kb_corpus.iloc[best_kb_idx]
                
                df.at[i, "kb_applied_label"] = best_kb_row.get("label", "")
                df.at[i, "kb_applied"] = True
                df.at[i, "kb_from_case"] = str(best_kb_row.get("case_id", ""))
                df.at[i, "kb_similarity_score"] = float(best_score)
                df.at[i, "kb_similarity_ip"] = float(ip_scores[best_idx_in_candidates])
                df.at[i, "kb_similarity_cosine"] = float(cos_scores[best_idx_in_candidates])
                df.at[i, "kb_similarity_overlap"] = float(overlap_scores[best_idx_in_candidates])
                applied += 1
            
            # ì§„í–‰ë¥  ë¡œê¹…
            if (i + 1) % 1000 == 0:
                log.info(f"  ì§„í–‰: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
        
        return applied, total

    def _build_composite_vectors_batch(self, 
                                       query_vectors: np.ndarray,
                                       query_shaps: List[Dict]) -> np.ndarray:
        """ë³µí•© ë²¡í„° ë°°ì¹˜ ìƒì„±"""
        n = len(query_vectors)
        
        # Feature ì •ê·œí™” (ë°°ì¹˜)
        if self.kb_instance.scaler is not None:
            normalized = self.kb_instance.scaler.transform(query_vectors).astype(np.float32)
        else:
            normalized = query_vectors
        
        # SHAP One-Hot (ë°°ì¹˜)
        vocab_size = len(self.kb_instance.shap_feature_vocab)
        shap_onehot = np.zeros((n, vocab_size), dtype=np.float32)
        
        for i, shap_dict in enumerate(query_shaps):
            for feat_name in shap_dict.keys():
                if feat_name in self.kb_instance.shap_feature_vocab:
                    feat_idx = self.kb_instance.shap_feature_vocab.index(feat_name)
                    shap_onehot[i, feat_idx] = 1.0
        
        # ë³µí•© ë²¡í„°
        return np.hstack([normalized, shap_onehot]).astype(np.float32)

    def _run_fallback(self, df: pd.DataFrame, common: List[str], gamma: float) -> ToolResult:
        """Fallback: FAISS ì—†ì„ ë•Œ"""
        log.warning("FAISS ì—†ìŒ - ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
        # ê¸°ì¡´ Brute-Force ë¡œì§...
        return ToolResult(True, "fallback", output_path=None)