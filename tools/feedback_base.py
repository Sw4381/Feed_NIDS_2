# -*- coding: utf-8 -*-
"""
Feedback Base Manager with FAISS Vector Store Integration
Reviewed Feedback ì¼€ì´ìŠ¤ë“¤ì„ ë¡œë“œí•˜ê³  FAISS ë²¡í„° ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆ
- ì§€ì†ì  ì¦ì¶• (reviewed=True ì¼€ì´ìŠ¤ ëˆ„ì )
- í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ì €ìž¥ (feedback_reason ë“±)
- LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ì§€ì›
"""
from __future__ import annotations
import os
import glob
import pickle
import hashlib
import time
from typing import Optional, List, Dict, Tuple
import numpy as np
import pandas as pd
from tools.base import get_logger

log = get_logger("FeedbackBase")

# ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ Feature 22ê°œ (KBì™€ ë™ì¼)
NETWORK_SECURITY_FEATURES = [
    'alpahbet_cnt_payload_sum',
    'alpha_cnt_dns_query_sum',
    'client_extensions_cnt',
    'entropys_avg',
    'flow_delta_times_sum',
    'flow_duration_seconds',
    'flow_stdev_time',
    'nonascii_cnt_dns_query_sum',
    'nonascii_cnt_payload_sum',
    'number_cnt_dns_query_sum',
    'number_cnt_payload_sum',
    'payload_len_max',
    'payload_len_min',
    'payload_lens_sum',
    'payload_packets_cnt',
    'query_response_ttls_sum',
    'server_certificates_cnt',
    'server_extensions_cnt',
    'special_cnt_dns_query_sum',
    'special_cnt_payload_sum',
    'tls_SAN_cnt',
    'total_packets_cnt'
]

# FAISS ê°€ìš©ì„± ì²´í¬
try:
    import faiss
    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False
    log.warning("âš ï¸ FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ê¸°ëŠ¥ ë¹„í™œì„±í™” (pip install faiss-cpu)")

# StandardScaler
try:
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    log.warning("âš ï¸ scikit-learn ë¯¸ì„¤ì¹˜ - ì •ê·œí™” ì—†ì´ ì§„í–‰ (pip install scikit-learn)")


class FeedbackBase:
    """
    Feedback ì½”í¼ìŠ¤ ê´€ë¦¬ (FAISS + í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°)
    
    Features:
    - reviewed=True ì¼€ì´ìŠ¤ë§Œ ìžë™ ë¡œë“œ ë° ì¦ì¶•
    - FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
    - í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´ (feedback_reason, feedback_label ë“±)
    - ìºì‹œ ì‹œìŠ¤í…œ (hash ê¸°ë°˜, ìžë™ ìž¬êµ¬ì¶•)
    - LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ì§€ì›
    - GPU ì§€ì› (ì„ íƒ ì‚¬í•­)
    """
    
    def __init__(self, 
                 feedback_dir: str = "./feedback_cases",
                 use_faiss: bool = True,
                 cache_dir: str = "./cache",
                 index_type: str = "IVF",
                 n_clusters: int = 100,
                 use_gpu: bool = False,
                 auto_rebuild: bool = True):
        """
        Args:
            feedback_dir: Feedback Cases ë””ë ‰í† ë¦¬ ê²½ë¡œ
            use_faiss: FAISS ë²¡í„° ì¸ë±ìŠ¤ ì‚¬ìš© ì—¬ë¶€
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
            index_type: FAISS ì¸ë±ìŠ¤ íƒ€ìž… ("Flat", "IVF", "HNSW")
            n_clusters: IVF í´ëŸ¬ìŠ¤í„° ìˆ˜
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            auto_rebuild: ìƒˆ í”¼ë“œë°± ê°ì§€ì‹œ ìžë™ ìž¬êµ¬ì¶•
        """
        self.feedback_dir = feedback_dir
        self.fb_df = None
        self.is_loaded = False
        
        # FAISS ì„¤ì •
        self.use_faiss = use_faiss and HAS_FAISS
        self.cache_dir = cache_dir
        self.index_type = index_type
        self.n_clusters = n_clusters
        self.use_gpu = use_gpu and HAS_FAISS
        self.auto_rebuild = auto_rebuild
        
        # FAISS ë²¡í„° ì €ìž¥ì†Œ
        self.faiss_index = None
        self.scaler = None
        self.feature_columns = []
        self.shap_feature_vocab = []
        self.metadata = []
        self.faiss_built = False
        
        # GPU ë¦¬ì†ŒìŠ¤ (ìž¬ì‚¬ìš©)
        self.gpu_resources = None
        
        if self.use_faiss:
            os.makedirs(cache_dir, exist_ok=True)

    def load(self, force_rebuild: bool = False) -> bool:
        """
        Feedback ë””ë ‰í† ë¦¬ì—ì„œ reviewed=True ì¼€ì´ìŠ¤ë§Œ ë¡œë“œ
        FAISSê°€ í™œì„±í™”ë˜ì–´ ìžˆìœ¼ë©´ ìžë™ìœ¼ë¡œ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Args:
            force_rebuild: ê°•ì œ ìž¬êµ¬ì¶• ì—¬ë¶€
            
        Returns: ì„±ê³µ ì—¬ë¶€
        """
        if not os.path.exists(self.feedback_dir):
            log.warning(f"Feedback ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.feedback_dir}")
            return False

        # ëª¨ë“  *_low_confidence_cases.csv íŒŒì¼ ìˆ˜ì§‘
        files = sorted(glob.glob(os.path.join(self.feedback_dir, "*_low_confidence_cases.csv")))
        if not files:
            log.warning(f"Feedback ë””ë ‰í† ë¦¬ì— CSV íŒŒì¼ ì—†ìŒ: {self.feedback_dir}")
            return False

        dfs = []
        for f in files:
            try:
                df = pd.read_csv(f, low_memory=False)
                
                # reviewed=Trueì´ê³  feedback_labelì´ ìžˆëŠ” ì¼€ì´ìŠ¤ë§Œ
                if "reviewed" not in df.columns or "feedback_label" not in df.columns:
                    continue
                
                reviewed = df[
                    (df["reviewed"] == True) & 
                    (df["feedback_label"].notna()) & 
                    (df["feedback_label"] != "")
                ].copy()
                
                if len(reviewed) > 0:
                    # ì†ŒìŠ¤ íŒŒì¼ ì •ë³´ ì¶”ê°€
                    reviewed["__fb_source"] = os.path.basename(f)
                    dfs.append(reviewed)
                    log.info(f"ë¡œë“œ: {os.path.basename(f)} ({len(reviewed)} reviewed cases)")
                    
            except Exception as e:
                log.warning(f"ë¡œë“œ ì‹¤íŒ¨: {f} â†’ {e}")
                continue

        if not dfs:
            log.warning("ë¡œë“œëœ reviewed ì¼€ì´ìŠ¤ ì—†ìŒ")
            return False

        self.fb_df = pd.concat(dfs, ignore_index=True)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required = ["feedback_label", "case_id"]
        missing = [c for c in required if c not in self.fb_df.columns]
        if missing:
            log.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
            return False
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        if "feedback_reason" not in self.fb_df.columns:
            self.fb_df["feedback_reason"] = ""
        if "feedback_confidence" not in self.fb_df.columns:
            self.fb_df["feedback_confidence"] = 0

        self.is_loaded = True
        log.info(f"âœ… Feedback Base ë¡œë“œ ì™„ë£Œ: {len(self.fb_df)} reviewed cases")
        
        # FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶•
        if self.use_faiss:
            log.info("\n" + "=" * 70)
            log.info("FAISS ë²¡í„° ì¸ë±ìŠ¤ ìžë™ êµ¬ì¶• ì‹œìž‘")
            log.info("=" * 70)
            
            # ìºì‹œ í™•ì¸
            data_hash = self._compute_data_hash(self.fb_df)
            cache_paths = self._get_cache_paths(data_hash)
            
            # ìºì‹œ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸
            cache_exists = all(os.path.exists(p) for p in cache_paths.values())
            
            if cache_exists and not force_rebuild and not self.auto_rebuild:
                log.info(f"ðŸ“¦ ìºì‹œì—ì„œ ë¡œë“œ (hash={data_hash[:8]})")
                try:
                    success = self._load_from_cache(cache_paths)
                    if success:
                        log.info("âœ… FAISS ë²¡í„° ì¸ë±ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ")
                        return True
                except Exception as e:
                    log.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ìž¬êµ¬ì¶•: {e}")
            
            # ìžë™ ìž¬êµ¬ì¶• ë¡œì§
            if self.auto_rebuild and cache_exists:
                cached_hash = self._load_cached_hash(cache_paths)
                if cached_hash != data_hash:
                    log.info(f"ðŸ”¨ ìƒˆ í”¼ë“œë°± ê°ì§€ (hash ë³€ê²½) â†’ ìž¬êµ¬ì¶•")
                    force_rebuild = True
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            success = self.build_faiss_index(force_rebuild=force_rebuild)
            if success:
                log.info("âœ… FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            else:
                log.warning("âš ï¸ FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨ - ê¸°ë³¸ Feedbackë§Œ ì‚¬ìš©")
        
        return True

    def build_faiss_index(self, feature_cols: List[str] = None, 
                         top_k: int = 5, force_rebuild: bool = False) -> bool:
        """
        FAISS ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (22ê°œ Feature + SHAP Top-5)
        KBì™€ ë™ì¼í•œ êµ¬ì¡°
        """
        if not self.is_loaded or self.fb_df is None:
            log.error("Feedback Baseê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return False
        
        if not HAS_FAISS:
            log.warning("FAISS ë¯¸ì„¤ì¹˜ - ë²¡í„° ì¸ë±ìŠ¤ ìŠ¤í‚µ")
            return False
        
        # Feature ì»¬ëŸ¼ ê²°ì •
        if feature_cols is None:
            feature_cols = NETWORK_SECURITY_FEATURES
        
        available_features = [f for f in feature_cols if f in self.fb_df.columns]
        missing_features = [f for f in feature_cols if f not in self.fb_df.columns]
        
        if missing_features:
            log.warning(f"ëˆ„ë½ëœ Feature ({len(missing_features)}ê°œ)")
        
        if not available_features:
            log.error("ì‚¬ìš© ê°€ëŠ¥í•œ Feature ì—†ìŒ")
            return False
        
        log.info(f"ðŸ“Š ì‚¬ìš© Feature: {len(available_features)}/{len(feature_cols)}")
        log.info(f"ðŸŽ® GPU ì‚¬ìš©: {'ON' if self.use_gpu else 'OFF'}")
        log.info(f"ðŸ”§ ì¸ë±ìŠ¤ íƒ€ìž…: {self.index_type}")
        log.info(f"ðŸ“¦ Feedback í¬ê¸°: {len(self.fb_df)}ê°œ")
        
        start_total = time.time()
        
        # Step 1: Feature ë²¡í„° ì¶”ì¶œ ë° ì •ê·œí™”
        log.info(f"  Step 1/5: Feature ì •ê·œí™”...")
        start = time.time()
        
        feature_matrix = self.fb_df[available_features].values.astype(np.float32)
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)
        
        if HAS_SKLEARN:
            self.scaler = StandardScaler()
            normalized_features = self.scaler.fit_transform(feature_matrix).astype(np.float32)
        else:
            self.scaler = None
            normalized_features = feature_matrix
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 2: SHAP One-Hot ì¸ì½”ë”© (ë²¡í„°í™”)
        log.info(f"  Step 2/5: SHAP One-Hot ì¸ì½”ë”© (ë²¡í„°í™”)...")
        start = time.time()
        
        self.shap_feature_vocab = NETWORK_SECURITY_FEATURES
        shap_to_idx = {feat: i for i, feat in enumerate(self.shap_feature_vocab)}
        
        # ë¹ˆ í–‰ë ¬ ìƒì„±
        shap_onehot_matrix = np.zeros((len(self.fb_df), 22), dtype=np.float32)
        
        # ê° SHAP Top-K ì»¬ëŸ¼ì„ ë²¡í„°í™” ì²˜ë¦¬
        for rank in range(1, top_k + 1):
            feat_col = f"shap_top{rank}_feature"
            
            if feat_col not in self.fb_df.columns:
                continue
            
            # ì „ì²´ ì»¬ëŸ¼ì„ í•œë²ˆì— ê°€ì ¸ì˜´
            feat_series = self.fb_df[feat_col].astype(str)
            
            # ìœ íš¨í•œ ê°’ë§Œ í•„í„°ë§
            valid_mask = ~feat_series.isin(["", "nan", "None", "NaN"])
            
            # ê° Featureì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í–‰ ì°¾ê¸°
            for feat_name, feat_idx in shap_to_idx.items():
                match_mask = valid_mask & (feat_series == feat_name)
                matching_rows = self.fb_df.index[match_mask].to_numpy()
                
                # One-Hot ì„¤ì •
                if len(matching_rows) > 0:
                    shap_onehot_matrix[matching_rows, feat_idx] = 1.0
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 3: ë³µí•© ë²¡í„° ìƒì„±
        log.info(f"  Step 3/5: ë³µí•© ë²¡í„° ìƒì„±...")
        start = time.time()
        
        composite_vectors = np.hstack([normalized_features, shap_onehot_matrix]).astype(np.float32)
        
        log.info(f"    â”œâ”€ Feature ì°¨ì›: {normalized_features.shape[1]}")
        log.info(f"    â”œâ”€ SHAP ì°¨ì›: {shap_onehot_matrix.shape[1]} (22ê°œ ê³ ì •)")
        log.info(f"    â””â”€ ì´ ì°¨ì›: {composite_vectors.shape[1]}")
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 4: ë©”íƒ€ë°ì´í„° êµ¬ì¶• (í…ìŠ¤íŠ¸ í¬í•¨)
        log.info(f"  Step 4/5: ë©”íƒ€ë°ì´í„° êµ¬ì¶• (í…ìŠ¤íŠ¸ í¬í•¨)...")
        start = time.time()
        
        self.metadata = []
        
        # SHAP ì»¬ëŸ¼ë“¤
        shap_cols = [f"shap_top{i}_feature" for i in range(1, top_k + 1) 
                    if f"shap_top{i}_feature" in self.fb_df.columns]
        
        for idx in range(len(self.fb_df)):
            row = self.fb_df.iloc[idx]
            
            meta = {
                "idx": idx,
                "case_id": str(row.get("case_id", "")),
                "A_ip": str(row.get("A_ip", "")),
                "B_ip": str(row.get("B_ip", "")),
                
                # âœ… Feedback ê³ ìœ  í•„ë“œ
                "feedback_label": str(row.get("feedback_label", "")),
                "feedback_reason": str(row.get("feedback_reason", "")),  # í…ìŠ¤íŠ¸
                "feedback_confidence": int(row.get("feedback_confidence", 0)),
                "reviewed_by": str(row.get("reviewed_by", "")),
                "review_date": str(row.get("review_date", "")),
                
                # ì†ŒìŠ¤ ì •ë³´
                "source_file": str(row.get("__fb_source", "")),
                
                # SHAP features
                "shap_features": {}
            }
            
            # SHAP features ì¶”ì¶œ
            for rank, col in enumerate(shap_cols, 1):
                feat_name = str(row[col])
                if feat_name and feat_name not in ("", "nan", "None"):
                    if feat_name in shap_to_idx:
                        meta["shap_features"][feat_name] = rank
            
            self.metadata.append(meta)
        
        log.info(f"    âœ… ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # Step 5: FAISS ì¸ë±ìŠ¤ ìƒì„±
        log.info(f"  Step 5/5: FAISS ì¸ë±ìŠ¤ ìƒì„±...")
        start = time.time()
        
        dimension = composite_vectors.shape[1]
        self.feature_columns = available_features
        
        self.faiss_index = self._create_faiss_index(dimension)
        
        log.info(f"    âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ ({time.time()-start:.2f}ì´ˆ)")
        
        # IVF Train
        if self.index_type == "IVF":
            log.info(f"    ðŸ”§ IVF í•™ìŠµ ì‹œìž‘ (GPU={'ON' if self.use_gpu else 'OFF'})...")
            start = time.time()
            
            self.faiss_index.train(composite_vectors)
            
            elapsed = time.time() - start
            log.info(f"    âœ… IVF í•™ìŠµ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")
        
        # ë²¡í„° ì¶”ê°€
        log.info(f"    ðŸ”§ ë²¡í„° ì¶”ê°€ ì‹œìž‘ ({len(composite_vectors)}ê°œ)...")
        start = time.time()
        
        self.faiss_index.add(composite_vectors)
        
        elapsed = time.time() - start
        vec_per_sec = len(composite_vectors) / elapsed if elapsed > 0 else 0
        log.info(f"    âœ… ë²¡í„° ì¶”ê°€ ì™„ë£Œ ({elapsed:.2f}ì´ˆ, {vec_per_sec:.0f} vec/s)")
        
        self.faiss_built = True
        
        # ìºì‹œ ì €ìž¥
        data_hash = self._compute_data_hash(self.fb_df)
        cache_paths = self._get_cache_paths(data_hash)
        
        try:
            self._save_to_cache(cache_paths, composite_vectors, data_hash)
            log.info(f"    ðŸ’¾ ìºì‹œ ì €ìž¥ ì™„ë£Œ")
        except Exception as e:
            log.warning(f"    âš ï¸ ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")
        
        total_elapsed = time.time() - start_total
        log.info(f"\nâœ… FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.metadata)}ê°œ ì¼€ì´ìŠ¤ ({total_elapsed:.2f}ì´ˆ)")
        
        return True

    def search_similar_cases(self, 
                            query_features: np.ndarray, 
                            query_shap_features: Dict[str, int],
                            k: int = 10) -> List[Tuple[int, float, Dict]]:
        """
        FAISSë¡œ ìœ ì‚¬ Feedback ì¼€ì´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query_features: ì¿¼ë¦¬ feature ë²¡í„° (ì •ê·œí™” ì „)
            query_shap_features: ì¿¼ë¦¬ SHAP features {name: rank}
            k: ë°˜í™˜í•  í›„ë³´ ìˆ˜
            
        Returns:
            [(fb_index, distance, metadata), ...]
            metadataì—ëŠ” feedback_reason, feedback_label ë“± í¬í•¨
        """
        if not self.faiss_built:
            log.warning("FAISS ì¸ë±ìŠ¤ ë¯¸êµ¬ì¶•")
            return []
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        query_features = np.nan_to_num(query_features, nan=0.0).astype(np.float32)
        if self.scaler is not None:
            normalized_query = self.scaler.transform(query_features.reshape(1, -1)).astype(np.float32)
        else:
            normalized_query = query_features.reshape(1, -1).astype(np.float32)
        
        # SHAP One-Hot (22ê°œ ê³ ì •)
        query_shap_onehot = np.zeros((1, 22), dtype=np.float32)
        shap_to_idx = {feat: i for i, feat in enumerate(self.shap_feature_vocab)}
        
        for feat_name in query_shap_features.keys():
            if feat_name in shap_to_idx:
                feat_idx = shap_to_idx[feat_name]
                query_shap_onehot[0, feat_idx] = 1.0
        
        # ë³µí•© ë²¡í„°
        query_composite = np.hstack([normalized_query, query_shap_onehot]).astype(np.float32)
        
        # ê²€ìƒ‰
        if self.index_type == "IVF":
            if hasattr(self.faiss_index, 'nprobe'):
                self.faiss_index.nprobe = min(10, self.n_clusters)
        
        distances, indices = self.faiss_index.search(query_composite, k)
        
        results = []
        for i in range(len(indices[0])):
            idx = int(indices[0][i])
            dist = float(distances[0][i])
            if idx >= 0 and idx < len(self.metadata):
                results.append((idx, dist, self.metadata[idx]))
        
        return results

    def generate_llm_context(self, 
                            similar_cases: List[Tuple[int, float, Dict]],
                            max_cases: int = 3) -> str:
        """
        ê²€ìƒ‰ëœ ìœ ì‚¬ ì‚¬ë¡€ë“¤ì„ LLM í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        
        Args:
            similar_cases: search_similar_cases() ê²°ê³¼
            max_cases: ìµœëŒ€ í¬í•¨ ì¼€ì´ìŠ¤ ìˆ˜
            
        Returns:
            LLM í”„ë¡¬í”„íŠ¸ ë¬¸ìžì—´
        """
        if not similar_cases:
            return ""
        
        context_parts = ["## ìœ ì‚¬ í”¼ë“œë°± ì‚¬ë¡€ ë¶„ì„\n"]
        
        for i, (idx, distance, meta) in enumerate(similar_cases[:max_cases], 1):
            # ìœ ì‚¬ë„ ì ìˆ˜ (ê±°ë¦¬ë¥¼ 0~1 ì ìˆ˜ë¡œ ë³€í™˜)
            similarity_score = 1.0 / (1.0 + distance)
            
            case_text = f"""
### ì‚¬ë¡€ {i} (ìœ ì‚¬ë„: {similarity_score:.3f})
- **íŒì •**: {meta.get('feedback_label', 'Unknown')}
- **ì‹ ë¢°ë„**: {meta.get('feedback_confidence', 0)}/5
- **ë¶„ì„ ë‚´ìš©**: {meta.get('feedback_reason', '(ì„¤ëª… ì—†ìŒ)')}
- **ë„¤íŠ¸ì›Œí¬**: {meta.get('A_ip', 'N/A')} â†’ {meta.get('B_ip', 'N/A')}
- **ì¼€ì´ìŠ¤ ID**: {meta.get('case_id', 'N/A')}
- **ê²€í†  ë‚ ì§œ**: {meta.get('review_date', 'N/A')}
"""
            context_parts.append(case_text.strip())
        
        return "\n".join(context_parts)

    def get_stats(self) -> Dict[str, any]:
        """Feedback Base í†µê³„ ë°˜í™˜"""
        if not self.is_loaded or self.fb_df is None:
            return {}
        
        stats = {
            "total": len(self.fb_df),
            "labels": self.fb_df["feedback_label"].value_counts().to_dict() if "feedback_label" in self.fb_df.columns else {},
            "files_loaded": len(glob.glob(os.path.join(self.feedback_dir, "*_low_confidence_cases.csv"))),
            "faiss_enabled": self.faiss_built,
            "gpu_enabled": self.use_gpu,
        }
        
        if self.faiss_built:
            stats["faiss_index_type"] = self.index_type
            stats["faiss_dimension"] = len(self.feature_columns) + 22
            stats["feature_dim"] = len(self.feature_columns)
            stats["shap_vocab_dim"] = 22
        
        # Confidence ë¶„í¬
        if "feedback_confidence" in self.fb_df.columns:
            stats["confidence_distribution"] = self.fb_df["feedback_confidence"].value_counts().to_dict()
        
        return stats

    # ========================================
    # FAISS ì¸ë±ìŠ¤ ìƒì„± (KBì™€ ë™ì¼)
    # ========================================
    
    def _create_faiss_index(self, dimension: int):
        """FAISS ì¸ë±ìŠ¤ ìƒì„± (GPU ì „ì†¡ í¬í•¨)"""
        # CPU ì¸ë±ìŠ¤ ìƒì„±
        if self.index_type == "Flat":
            index = faiss.IndexFlatL2(dimension)
        elif self.index_type == "IVF":
            quantizer = faiss.IndexFlatL2(dimension)
            n_clusters = min(self.n_clusters, max(1, len(self.metadata) // 10))
            index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)
        elif self.index_type == "HNSW":
            index = faiss.IndexHNSWFlat(dimension, 32)
            index.hnsw.efConstruction = 40
        else:
            index = faiss.IndexFlatL2(dimension)
        
        # GPU ì „ì†¡
        if self.use_gpu:
            try:
                if self.gpu_resources is None:
                    self.gpu_resources = faiss.StandardGpuResources()
                    self.gpu_resources.setTempMemory(1024 * 1024 * 1024)  # 1GB
                
                index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, index)
                log.info(f"      âœ… GPUë¡œ ì¸ë±ìŠ¤ ì „ì†¡ ì„±ê³µ (Device 0)")
                
            except Exception as e:
                log.error(f"      âŒ GPU ì „ì†¡ ì‹¤íŒ¨: {e}")
                log.error(f"      âš ï¸ CPU ëª¨ë“œë¡œ fallbackí•©ë‹ˆë‹¤")
                self.use_gpu = False
        
        return index

    # ========================================
    # ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    # ========================================
    
    def _compute_data_hash(self, fb_df: pd.DataFrame) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        key_parts = [
            f"rows:{len(fb_df)}",
            f"cols:{len(self.feature_columns) if self.feature_columns else 0}",
        ]
        
        # case_id ê¸°ë°˜ í•´ì‹œ (reviewed ì¼€ì´ìŠ¤ ì¶”ì )
        if "case_id" in fb_df.columns:
            case_ids = sorted(fb_df["case_id"].astype(str).unique())
            case_hash = hashlib.md5(",".join(case_ids).encode()).hexdigest()[:16]
            key_parts.append(f"cases:{case_hash}")
        
        # review_date ê¸°ë°˜ í•´ì‹œ (ìµœì‹ ì„± ì¶”ì )
        if "review_date" in fb_df.columns:
            dates = fb_df["review_date"].astype(str).unique()
            date_hash = hashlib.md5(",".join(sorted(dates)).encode()).hexdigest()[:8]
            key_parts.append(f"dates:{date_hash}")
        
        return hashlib.md5("_".join(key_parts).encode()).hexdigest()
    
    def _get_cache_paths(self, data_hash: str) -> Dict[str, str]:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
        prefix = os.path.join(self.cache_dir, f"feedback_{data_hash}")
        return {
            "index": f"{prefix}_index.faiss",
            "scaler": f"{prefix}_scaler.pkl",
            "metadata": f"{prefix}_metadata.pkl",
            "config": f"{prefix}_config.pkl",
            "hash": f"{prefix}_hash.txt",
        }
    
    def _save_to_cache(self, cache_paths: Dict[str, str], vectors: np.ndarray, data_hash: str):
        """ìºì‹œ ì €ìž¥"""
        # GPU ì¸ë±ìŠ¤ëŠ” CPUë¡œ ë³€í™˜ í›„ ì €ìž¥
        if self.use_gpu:
            cpu_index = faiss.index_gpu_to_cpu(self.faiss_index)
            faiss.write_index(cpu_index, cache_paths["index"])
        else:
            faiss.write_index(self.faiss_index, cache_paths["index"])
        
        if self.scaler is not None:
            with open(cache_paths["scaler"], "wb") as f:
                pickle.dump(self.scaler, f)
        
        with open(cache_paths["metadata"], "wb") as f:
            pickle.dump(self.metadata, f)
        
        config = {
            "feature_columns": self.feature_columns,
            "shap_feature_vocab": self.shap_feature_vocab,
            "index_type": self.index_type,
            "n_clusters": self.n_clusters,
        }
        with open(cache_paths["config"], "wb") as f:
            pickle.dump(config, f)
        
        # í•´ì‹œ ì €ìž¥
        with open(cache_paths["hash"], "w") as f:
            f.write(data_hash)
    
    def _load_from_cache(self, cache_paths: Dict[str, str]) -> bool:
        """ìºì‹œ ë¡œë“œ"""
        with open(cache_paths["config"], "rb") as f:
            config = pickle.load(f)
        
        self.feature_columns = config["feature_columns"]
        self.shap_feature_vocab = config.get("shap_feature_vocab", NETWORK_SECURITY_FEATURES)
        
        self.faiss_index = faiss.read_index(cache_paths["index"])
        
        # GPUë¡œ ì „ì†¡
        if self.use_gpu:
            try:
                if self.gpu_resources is None:
                    self.gpu_resources = faiss.StandardGpuResources()
                    self.gpu_resources.setTempMemory(1024 * 1024 * 1024)
                
                self.faiss_index = faiss.index_cpu_to_gpu(self.gpu_resources, 0, self.faiss_index)
                log.info("âœ… ìºì‹œëœ ì¸ë±ìŠ¤ë¥¼ GPUë¡œ ì „ì†¡ ì™„ë£Œ")
            except Exception as e:
                log.warning(f"âš ï¸ GPU ì „ì†¡ ì‹¤íŒ¨, CPUë¡œ ì‚¬ìš©: {e}")
                self.use_gpu = False
        
        if os.path.exists(cache_paths["scaler"]):
            with open(cache_paths["scaler"], "rb") as f:
                self.scaler = pickle.load(f)
        
        with open(cache_paths["metadata"], "rb") as f:
            self.metadata = pickle.load(f)
        
        self.faiss_built = True
        return True
    
    def _load_cached_hash(self, cache_paths: Dict[str, str]) -> Optional[str]:
        """ìºì‹œëœ í•´ì‹œ ë¡œë“œ"""
        hash_file = cache_paths.get("hash")
        if hash_file and os.path.exists(hash_file):
            with open(hash_file, "r") as f:
                return f.read().strip()
        return None

    def __len__(self) -> int:
        return len(self.fb_df) if self.is_loaded and self.fb_df is not None else 0

    def __repr__(self) -> str:
        if not self.is_loaded:
            return "FeedbackBase(not loaded)"
        stats = self.get_stats()
        gpu_info = f", GPU={'ON' if self.use_gpu else 'OFF'}" if self.faiss_built else ""
        faiss_info = f", FAISS({self.index_type}){gpu_info}" if self.faiss_built else ""
        return f"FeedbackBase(total={stats.get('total', 0)}, labels={stats.get('labels', {})}{faiss_info})"